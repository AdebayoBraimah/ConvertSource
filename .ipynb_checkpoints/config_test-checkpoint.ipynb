{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "import subprocess\n",
    "import pathlib\n",
    "import yaml\n",
    "import nibabel as nib\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = \"C:/Users/smart/Desktop/GitProjects/convsauce/ConvertSource/cfg.test.yml\"\n",
    "cfg = \"/Users/brac4g/Desktop/convsauce/ConvertSource/cfg.test.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(config_file, verbose = False):\n",
    "    '''\n",
    "    Reads configuration file and creates a dictionary of search terms for \n",
    "    certain modalities provided that BIDS modalities are used as keys. If\n",
    "    exclusions are provided (via the key 'exclude') then an exclusion list is \n",
    "    created. Otherwise, 'exclusion_list' is returned as an empty list. If \n",
    "    additional settings are specified, they should be done so via the key\n",
    "    'settings' to enable writing of additional metadata.\n",
    "    \n",
    "    Arguments:\n",
    "        config_file (string): file path to yaml configuration file.\n",
    "        verbose (boolean): Prints additional information to screen.\n",
    "    \n",
    "    Returns: \n",
    "        data_map (dict): Nested dictionary of search terms for BIDS modalities\n",
    "        exclusion_list (list): List of exclusion terms\n",
    "        settings_dict (dict): Nested dictionary of metadata terms to write to JSON file(s)\n",
    "    '''\n",
    "    \n",
    "    with open(config_file) as file:\n",
    "        data_map = yaml.safe_load(file)\n",
    "        if verbose:\n",
    "            print(\"Initialized parameters from configuration file\")\n",
    "        \n",
    "    if any(\"exclude\" in data_map for element in data_map):\n",
    "        if verbose:\n",
    "            print(\"exclusion option implemented\")\n",
    "        exclusion_list = data_map[\"exclude\"]\n",
    "        del data_map[\"exclude\"]\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"exclusion option not implemented\")\n",
    "        exclusion_list = list()\n",
    "        \n",
    "    if any(\"metadata\" in data_map for element in data_map):\n",
    "        if verbose:\n",
    "            print(\"implementing additional settings for metadata\")\n",
    "        settings_dict = data_map[\"metadata\"]\n",
    "        del data_map[\"metadata\"]\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"no metadata settings\")\n",
    "        settings_dict = dict()\n",
    "        \n",
    "    return data_map,exclusion_list,settings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized parameters from configuration file\n",
      "exclusion option implemented\n",
      "implementing additional settings for metadata\n"
     ]
    }
   ],
   "source": [
    "search_dict, exclusion_list, param_dict = read_config(cfg,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anat': {'T1w': ['T1', 'T1w', 'TFE'], 'T2w': ['T2', 'T2w', 'TSE']},\n",
       " 'func': {'bold': {'rest': ['rsfMR', 'rest', 'FFE', 'FEEPI'],\n",
       "   'visualstrobe': ['vis', 'visual']}},\n",
       " 'fmap': {'fmap': ['map']},\n",
       " 'swi': {'swi': ['swi']},\n",
       " 'dwi': {'dwi': ['diffusion', 'DTI', 'DWI', '6_DIR']}}"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SURVEY',\n",
       " 'Reg',\n",
       " 'SHORT',\n",
       " 'LONG',\n",
       " 'MRS',\n",
       " 'PRESS',\n",
       " 'DEFAULT',\n",
       " 'ScreenCapture',\n",
       " 'PD',\n",
       " 'ALL',\n",
       " 'SPECTRO']"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclusion_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'general': {'Manufacturer': 'Philips',\n",
       "  'ManufacturersModelName': 'Ingenia',\n",
       "  'MagneticFieldStrength': 3,\n",
       "  'InstitutionName': \"Cincinnati Children's Hospital Medical Center\",\n",
       "  'ParallelAcquisitionTechnique': 'SENSE'},\n",
       " 'func': {'rest': {'PhaseEncodingDirection': 'j'},\n",
       "  'visualstrobe': {'PhaseEncodingDirection': 'j'}},\n",
       " 'dwi': {'PhaseEncodingDirection': 'j'}}"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir_par = \"C:/Users/smart/Desktop/GitProjects/convsauce/287H_C10/PAR REC\"\n",
    "data_dir_par = \"/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir_dcm = \"C:/Users/smart/Desktop/GitProjects/convsauce/IRC287H-8/20171003\"\n",
    "data_dir_dcm = \"/Users/brac4g/Desktop/convsauce/IRC287H-8/20171003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_nii = \"/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dcm_files(dcm_dir):\n",
    "    '''\n",
    "    Creates a file list consisting of the first DICOM file in a parent DICOM directory. \n",
    "    A file list is then returned.\n",
    "    \n",
    "    Arguments:\n",
    "        dcm_dir (string): Absolute path to parent DICOM data directory\n",
    "\n",
    "    Returns: \n",
    "        dcm_files (list): List of DICOM filenames, complete with their absolute paths.\n",
    "    '''\n",
    "    \n",
    "    # Create directory list\n",
    "    dcm_dir = os.path.abspath(dcm_dir)\n",
    "    parent_dcm_dir = os.path.join(dcm_dir,'*')\n",
    "    dcm_dir_list = glob.glob(parent_dcm_dir, recursive=True)\n",
    "\n",
    "    # Initilized dcm_file list\n",
    "    dcm_files = list()\n",
    "    \n",
    "    # Iterate through files in the dicom directory list\n",
    "    for dir_ in dcm_dir_list:\n",
    "        # print(dir_)\n",
    "        for root, dirs, files in os.walk(dir_):\n",
    "            # print(files[0])\n",
    "            tmp_dcm_file = files[0] # only need the first dicom file\n",
    "            tmp_dcm_dir = root\n",
    "            tmp_file = os.path.join(tmp_dcm_dir, tmp_dcm_file)\n",
    "\n",
    "            dcm_files.append(tmp_file)\n",
    "            break\n",
    "\n",
    "    return dcm_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_list(data_dir, file_ext=\"\", order=\"size\"):\n",
    "    '''\n",
    "    Creates a file list by globbing a directory for a specific file\n",
    "    extension and sorting by some determined order. A file list is \n",
    "    then returned\n",
    "    \n",
    "    Arguments:\n",
    "        data_dir (string): Absolute path to data directory (must be a directory dump of image data)\n",
    "        file_ext (string): File extension to glob. Built-in options include:\n",
    "            - 'par' or 'PAR': Searches for PAR headers\n",
    "            - 'dcm' or 'DICOM': Searches for DICOM directories, then searches for one file from each DICOM directory\n",
    "            - 'nii', or 'Nifti': Searches for nifti files (including gzipped nifti files)\n",
    "        order (string): Order to sort the list. Valid options are: 'size' and 'time':\n",
    "            - 'size': sorts by file size in ascending order (default)\n",
    "            - 'time': sorts by file modification time in ascending order\n",
    "            - 'none': no sorting is applied and the list is generated as the system finds the files\n",
    "    \n",
    "    Returns: \n",
    "        file_list (list): List of filenames, complete with their absolute paths.\n",
    "    '''\n",
    "    \n",
    "    # Check file extension\n",
    "    if file_ext != \"\":\n",
    "        if file_ext.upper() == \"PAR\" or file_ext.upper() == \"REC\":\n",
    "            file_ext = \"PAR\"\n",
    "            file_ext = f\".{file_ext.upper()}\"\n",
    "        elif file_ext.lower() == \"dcm\" or file_ext.upper() == \"DICOM\":\n",
    "            file_ext = \"dcm\"\n",
    "            file_ext = f\".{file_ext.lower()}\"\n",
    "        elif file_ext.lower() == \"nii\" or file_ext.lower() == \"nifti\":\n",
    "            file_ext = \"nii\"\n",
    "            file_ext = f\".{file_ext.lower()}*\" # Add wildcard for globbling gzipped files\n",
    "        else:\n",
    "            file_ext = f\".{file_ext}\"\n",
    "    \n",
    "    # Check sort order\n",
    "    if order.lower() == \"size\":\n",
    "        order_key = os.path.getsize\n",
    "    elif order.lower() == \"time\":\n",
    "        order_key = os.path.getmtime\n",
    "    elif order.lower() == \"none\":\n",
    "        order_key=None\n",
    "    else:\n",
    "        order_key = os.path.getsize\n",
    "        print(\"Unrecognized keyword option. Using default.\")\n",
    "    \n",
    "    # Create file list\n",
    "    if file_ext == \".dcm\":\n",
    "        file_list = sorted(get_dcm_files(data_dir), key=order_key, reverse=False)\n",
    "    elif file_ext != \".dcm\":\n",
    "        file_names = os.path.join(data_dir, f\"*{file_ext}\")\n",
    "        file_list = sorted(glob.glob(file_names, recursive=True), key=order_key, reverse=False)\n",
    "    \n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_file_list = create_file_list(data_dir=data_dir_par,file_ext=\"par\")\n",
    "dcm_file_list = create_file_list(data_dir=data_dir_dcm,file_ext=\"dcm\")\n",
    "nii_file_list = create_file_list(data_dir=data_dir_nii,file_ext=\"nii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exclude(file_list, data_dir, exclusion_list = [], verbose = False):\n",
    "    '''\n",
    "    Excludes files from the conversion process by removing filenames\n",
    "    that contain words that match those found in the 'exclusion_list'\n",
    "    from the 'read_config' function - should any files need/want to be \n",
    "    excluded.\n",
    "    \n",
    "    If 'exclusion_list' is empty, then the original 'file_list' is returned.\n",
    "    \n",
    "    Arguments:\n",
    "        file_list (list): List of filenames\n",
    "        data_dir (string): Absolute path to parent directory that contains the image data\n",
    "        exclusion_list (list): List of words to be matched. Filenames that contain these words will be excluded.\n",
    "        verbose (bool): Boolean - True or False.\n",
    "    \n",
    "    Returns: \n",
    "        currated_list (list): Currated list of filenames, with unwanted filenames removed.\n",
    "    '''\n",
    "            \n",
    "    # Check file extension in file list\n",
    "    if 'dcm' in file_list[0]:\n",
    "        file_ext = \"dcm\"\n",
    "        file_ext = f\".{file_ext.lower()}\"\n",
    "    elif 'PAR' in file_list[0]:\n",
    "        file_ext = \"PAR\"\n",
    "        file_ext = f\".{file_ext.upper()}\"\n",
    "    elif 'nii' in file_list[0]:\n",
    "        file_ext = \"nii\"\n",
    "        file_ext = f\".{file_ext.lower()}*\" # Add wildcard for globbling gzipped files\n",
    "    else:\n",
    "        file_ext = \"\"\n",
    "        file_ext = f\".{file_ext.lower()}\"\n",
    "    \n",
    "    # create set of lists\n",
    "    file_set = set(file_list)\n",
    "    \n",
    "    # create empty sets\n",
    "    currated_set = set()\n",
    "    exclusion_set = set()\n",
    "    \n",
    "    if len(exclusion_list) == 0:\n",
    "        currated_set = file_set\n",
    "    else:\n",
    "        for file in exclusion_list:\n",
    "            if file_ext == '.dcm':\n",
    "                dir_ = os.path.join(data_dir, f\"*{file}*\",f\"*{file_ext}\")\n",
    "            else:\n",
    "                dir_ = os.path.join(data_dir, f\"*{file}*{file_ext}\")\n",
    "            f_names = glob.glob(dir_, recursive=True)        \n",
    "            f_names_set = set(f_names)\n",
    "            exclusion_set.update(f_names_set)\n",
    "            \n",
    "        currated_set = file_set.difference(exclusion_set)\n",
    "\n",
    "    currated_list = list(currated_set)\n",
    "    \n",
    "    return currated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "par_file_list_currated = file_exclude(par_file_list,data_dir_par,exclusion_list)\n",
    "dcm_file_list_currated = file_exclude(dcm_file_list,data_dir_dcm,exclusion_list)\n",
    "nii_file_list_currated = file_exclude(nii_file_list,data_dir_nii,exclusion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_in_substr(sub_str_,str_):\n",
    "    '''\n",
    "    DEPRECATED: Should only be used if config_file uses comma separated\n",
    "        lists to denote search terms.\n",
    "    \n",
    "    Searches a (longer) string using a comma separated string \n",
    "    consisting of substrings. Returns 'True' or 'False' if any part\n",
    "    of the substring is found within the larger string.\n",
    "    \n",
    "    Example:\n",
    "        str_in_substr('T1,TFE','sub_T1_image_file') would return True.\n",
    "        str_in_substr('T2,TSE','sub_T1_image_file') would return False.\n",
    "    \n",
    "    Arguments:\n",
    "        sub_str_ (string): Substring used for matching.\n",
    "        str_ (string): Larger string to be searched for matches within substring.\n",
    "    \n",
    "    Returns: \n",
    "        bool_var (bool): Boolean - True or False\n",
    "    '''\n",
    "    \n",
    "    bool_var = False\n",
    "    \n",
    "    for word in sub_str_.split(\",\"):\n",
    "        if any(word in str_ for element in str_):\n",
    "            bool_var = True\n",
    "            \n",
    "    return bool_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_in_substr(list_,str_):\n",
    "    '''\n",
    "    Searches a string using a list that contains substrings. \n",
    "    Returns 'True' or 'False' if any elements of the list are \n",
    "    found within the string.\n",
    "    \n",
    "    Example:\n",
    "        list_in_substr('['T1','TFE']','sub_T1_image_file') would return True.\n",
    "        list_in_substr('['T2','TSE']','sub_T1_image_file') would return False.\n",
    "    \n",
    "    Arguments:\n",
    "        list_ (string): list containing strings used for matching.\n",
    "        str_ (string): Larger string to be searched for matches within substring.\n",
    "    \n",
    "    Returns: \n",
    "        bool_var (bool): Boolean - True or False\n",
    "    '''\n",
    "    \n",
    "    bool_var = False\n",
    "    \n",
    "    for word in list_:\n",
    "        if any(word.lower() in str_.lower() for element in str_.lower()):\n",
    "            bool_var = True\n",
    "            \n",
    "    return bool_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_mr(dcm_file, verbose=False):\n",
    "    '''\n",
    "    Checks for a valid DICOM file by inspecting the conversion type label in the DICOM file header.\n",
    "    This field should be blank. If this label is populated, then it is likely a secondary capture image \n",
    "    and thus is not likely to contain meaningful image information.\n",
    "    \n",
    "    Arguments:\n",
    "        dcm_file (string): DICOM filename with absolute filepath\n",
    "        verbose (boolean): Enable verbosity\n",
    "    \n",
    "    Returns: \n",
    "        is_valid (boolean): True if DICOM file is not a secondary capture (or does not have text in the conversion type label field)\n",
    "    '''\n",
    "    \n",
    "    # Read DICOM file header\n",
    "    ds = pydicom.dcmread(dcm_file)\n",
    "    \n",
    "    # Invalid files include secondary image captures, and are not suitable for \n",
    "    # nifti conversion as they are often not converted and cause problems.\n",
    "    # This string should be empty. If it is populated, then its likely a secondary capture.\n",
    "    conv_type = ds.ConversionType\n",
    "    \n",
    "    if conv_type in '':\n",
    "        is_valid = True\n",
    "    else:\n",
    "        is_valid = False\n",
    "        if verbose:\n",
    "            print(f\"Please check Conversion Type (0008, 0064) in dicom header. The presented DICOM file is not a valid file: {dcm_file}.\")\n",
    "    \n",
    "    return is_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scan_tech(dictionary, file, json_file=\"\"):\n",
    "    '''\n",
    "    Searches DICOM or PAR file header for scan technique/MR modality used in accordance with the search terms provided\n",
    "    by the nested dictionary.\n",
    "    \n",
    "    Note: This function is still undergoing active development.\n",
    "    \n",
    "    Arguments:\n",
    "        dictionary (dict): Nested dictionary from the 'read_config' function\n",
    "        dcm_file (string): Source image filename with absolute filepath\n",
    "    \n",
    "    Returns: \n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    # Check file extension in file list\n",
    "    if 'dcm' in file:\n",
    "        file_ext = \"dcm\"\n",
    "        file_ext = f\".{file_ext.lower()}\"\n",
    "    elif 'PAR' in file:\n",
    "        file_ext = \"PAR\"\n",
    "        file_ext = f\".{file_ext.upper()}\"\n",
    "    elif 'nii' in file:\n",
    "        file_ext = \"nii\"\n",
    "        file_ext = f\".{file_ext.lower()}\"\n",
    "    else:\n",
    "        file_ext = \"\"\n",
    "        file_ext = f\".{file_ext.lower()}\"\n",
    "    \n",
    "    # Perform Scanning Techniqe Search\n",
    "    if file_ext == '.dcm':\n",
    "        get_dcm_scan_tech(dictionary,file)\n",
    "    elif file_ext == '.PAR':\n",
    "        get_par_scan_tech(dictionary,file)\n",
    "    else:\n",
    "        print(\"unknown modality\")\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dcm_scan_tech(dictionary, dcm_file):\n",
    "    '''\n",
    "    Searches DICOM file header for scan technique/MR modality used in accordance with the search terms provided by the\n",
    "    nested dictionary. The DICOM header field searched is a Philips DICOM private tag (2001,1020) [Scanning Technique \n",
    "    Description MR]. In the case that field is does not match, is empty, or does not exist, then more common DICOM tags\n",
    "    are searched - and they include: Series Description, Protocol Name, and Image Type.\n",
    "    \n",
    "    Note: This function is still undergoing active development.\n",
    "    \n",
    "    Arguments:\n",
    "        dictionary (dict): Nested dictionary from the 'read_config' function\n",
    "        dcm_file (string): DICOM filename with absolute filepath\n",
    "    \n",
    "    Returns: \n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    mod_found = False\n",
    "    \n",
    "    # Load DICOM data and read header\n",
    "    ds = pydicom.dcmread(dcm_file)\n",
    "    \n",
    "    # Search DICOM header for Scan Technique used\n",
    "    dcm_scan_tech_str = str(ds[0x2001,0x1020])\n",
    "    \n",
    "    for key,item in dictionary.items():\n",
    "        for dict_key,dict_item in dictionary[key].items():\n",
    "            if isinstance(dict_item,list):\n",
    "                if list_in_substr(dict_item,dcm_scan_tech_str):\n",
    "                    mod_found = True\n",
    "                    print(f\"{key} - {dict_key}: {dict_item}\")\n",
    "                    if mod_found:\n",
    "                        break\n",
    "            elif isinstance(dict_item,dict):\n",
    "                tmp_dict = dictionary[key]\n",
    "                for d_key,d_item in tmp_dict[dict_key].items():\n",
    "                    if list_in_substr(d_item,dcm_scan_tech_str):\n",
    "                        mod_found = True\n",
    "                        print(f\"{key} - {dict_key} - {d_key}: {d_item}\")\n",
    "                        if mod_found:\n",
    "                            break\n",
    "                            \n",
    "        if mod_found:\n",
    "            break\n",
    "    \n",
    "    # Secondary look in the case Private Field (2001, 1020) [Scanning Technique Description MR] is empty\n",
    "    if not mod_found:\n",
    "        # Define list of DICOM header fields\n",
    "        dcm_fields = ['SeriesDescription', 'ImageType', 'ProtocolName']\n",
    "        \n",
    "        for dcm_field in dcm_fields:\n",
    "            dcm_scan_tech_str = str(eval(f\"ds.{dcm_field}\")) # This makes me dangerously uncomfortable\n",
    "            \n",
    "            for key,item in dictionary.items():\n",
    "                for dict_key,dict_item in dictionary[key].items():\n",
    "                    if isinstance(dict_item,list):\n",
    "                        if list_in_substr(dict_item,dcm_scan_tech_str):\n",
    "                            mod_found = True\n",
    "                            print(f\"{key} - {dict_key}: {dict_item}\")\n",
    "                            if mod_found:\n",
    "                                break\n",
    "                    elif isinstance(dict_item,dict):\n",
    "                        tmp_dict = dictionary[key]\n",
    "                        for d_key,d_item in tmp_dict[dict_key].items():\n",
    "                            if list_in_substr(d_item,dcm_scan_tech_str):\n",
    "                                mod_found = True\n",
    "                                print(f\"{key} - {dict_key} - {d_key}: {d_item}\")\n",
    "                                if mod_found:\n",
    "                                    break\n",
    "\n",
    "            if mod_found:\n",
    "                break\n",
    "                \n",
    "    if not mod_found:\n",
    "        print(\"unknown modality\")\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_par_scan_tech(dictionary, par_file):\n",
    "    '''\n",
    "    Searches PAR file header for scan technique/MR modality used in accordance with the search terms provided by the\n",
    "    nested dictionary. A regular expression (regEx) search string is defined and searched for conventional PAR headers.\n",
    "    \n",
    "    Note: This function is still undergoing active development.\n",
    "    \n",
    "    Arguments:\n",
    "        dictionary (dict): Nested dictionary from the 'read_config' function\n",
    "        par_file (string): PAR filename with absolute filepath\n",
    "    \n",
    "    Returns: \n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    mod_found = False\n",
    "    \n",
    "    # Define regEx search string\n",
    "    regexp = re.compile(r'.    Technique                          :  .*', re.M | re.I)\n",
    "    \n",
    "    # Open and search PAR header file\n",
    "    with open(par_file) as f:\n",
    "        for line in f:\n",
    "            match_ = regexp.match(line)\n",
    "            if match_:\n",
    "                par_scan_tech_str = match_.group()\n",
    "    \n",
    "    # Search Scan Technique with search terms\n",
    "    for key,item in dictionary.items():\n",
    "        for dict_key,dict_item in dictionary[key].items():\n",
    "            if isinstance(dict_item,list):\n",
    "                if list_in_substr(dict_item,par_scan_tech_str):\n",
    "                    mod_found = True\n",
    "                    print(f\"{key} - {dict_key}: {dict_item}\")\n",
    "                    if mod_found:\n",
    "                        break\n",
    "            elif isinstance(dict_item,dict):\n",
    "                tmp_dict = dictionary[key]\n",
    "                for d_key,d_item in tmp_dict[dict_key].items():\n",
    "                    if list_in_substr(d_item,par_scan_tech_str):\n",
    "                        mod_found = True\n",
    "                        print(f\"{key} - {dict_key} - {d_key}: {d_item}\")\n",
    "                        if mod_found:\n",
    "                            break\n",
    "                            \n",
    "        if mod_found:\n",
    "            break\n",
    "            \n",
    "    if not mod_found:\n",
    "        print(\"unknown modality\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_modality(dictionary, file, verbose=False):\n",
    "    '''\n",
    "    Converts an image file and extracts information from the filename\n",
    "    (such as the modality). \n",
    "    \n",
    "    Note: This function is still undergoing active development.\n",
    "    \n",
    "    Arguments:\n",
    "        dictionary (dict): Nested dictionary from the 'read_config' function\n",
    "        file (string): Filename with absolute filepath\n",
    "        verbose (boolean): Enable verbosity\n",
    "    \n",
    "    Returns: \n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    mod_found = False\n",
    "    \n",
    "    # Check file type\n",
    "    if 'nii' in file:\n",
    "        file_ext = \"nii\"\n",
    "        file_ext = f\".{file_ext.lower()}\"\n",
    "    elif 'dcm' in file:\n",
    "        file_ext = \"dcm\"\n",
    "        file_ext = f\".{file_ext.lower()}\"\n",
    "        if not is_valid_mr(file,verbose):\n",
    "            sys.exit(f\"Invalid DICOM file. Please check {file}\")\n",
    "    \n",
    "    for key,item in dictionary.items():\n",
    "        for dict_key,dict_item in dictionary[key].items():\n",
    "            if isinstance(dict_item,list):\n",
    "                if list_in_substr(dict_item,file):\n",
    "                    mod_found = True\n",
    "                    print(f\"{key} - {dict_key}: {dict_item}\")\n",
    "                    if mod_found:\n",
    "                        break\n",
    "            elif isinstance(dict_item,dict):\n",
    "                tmp_dict = dictionary[key]\n",
    "                for d_key,d_item in tmp_dict[dict_key].items():\n",
    "                    if list_in_substr(d_item,file):\n",
    "                        mod_found = True\n",
    "                        print(f\"{key} - {dict_key} - {d_key}: {d_item}\")\n",
    "                        if mod_found:\n",
    "                            break\n",
    "                        \n",
    "    if not mod_found:\n",
    "        get_scan_tech(dictionary,file)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_convert(file_list, dictionary, verbose=False):\n",
    "    '''\n",
    "    Batch conversion function for image files. \n",
    "    \n",
    "    Note: This function is still undergoing active development.\n",
    "    \n",
    "    Arguments:\n",
    "        file_list (list): List of filenames with absolute filepaths\n",
    "        dictionary (dict): Nested dictionary from the 'read_config' function\n",
    "        verbose (boolean): Enable verbosity\n",
    "    \n",
    "    Returns: \n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    for file in file_list:\n",
    "        try:\n",
    "            convert_modality(dictionary,file,verbose)\n",
    "        except SystemExit:\n",
    "            pass\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TaskName` JSON file appending funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_name = \"\"\n",
    "task_name = \"visualstrobe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_name is: visualstrobe\n"
     ]
    }
   ],
   "source": [
    "if task_name == \"\":\n",
    "    print(\"task_name is empty\")\n",
    "else:\n",
    "    print(f\"task_name is: {task_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "nii_file = \"c:/Users/smart/Desktop/GitProjects/convsauce/BIDS/rawdata/sub-C10/ses-001/func/sub-C10_ses-001_task-rest_acq-PA_run-01_bold.nii.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(nii_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = nib.load(nii_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nibabel.nifti1.Nifti1Image at 0x22eb4b28470>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 45, 400)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.header.get_data_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img.header.get_data_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = img.header.get_data_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 45, 400)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(frames[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `NifTi` File Conversion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_bids_anat(bids_out_dir, file, sub, scan, ses=1, scan_type='anat'):\n",
    "    '''\n",
    "    Renames converted nifti files to conform with BIDS format\n",
    "    (in the case of anatomical files).\n",
    "    NB: out_dir refers to the parent or RawData directory.\n",
    "    '''\n",
    "\n",
    "    # Create Output Directory Variables\n",
    "    # Zeropad subject ID if possible\n",
    "    try:\n",
    "        ses = '{:03}'.format(int(ses))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    # Zeropad session ID if possible\n",
    "    try:\n",
    "        ses = '{:03}'.format(int(ses))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    bids_out_dir = os.path.abspath(bids_out_dir)\n",
    "    outdir = os.path.join(out_dir, f\"sub-{sub}\", f\"ses-{ses}\", f\"{scan_type}\")\n",
    "\n",
    "    # Make output directory\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "\n",
    "    # Create temporary output names/directories\n",
    "    tmp_out_dir = os.path.join(out_dir, f\"sub-{sub}\", 'tmp_dir' + str(random.randint(0, n)))\n",
    "    tmp_basename = 'tmp_basename' + str(random.randint(0, n))\n",
    "\n",
    "    # Convert image file\n",
    "    # Check file extension in file list\n",
    "    if 'dcm' in file:\n",
    "        [nii_file, json_file] = convert_dcm_file(file, tmp_out_dir, tmp_basename)\n",
    "    elif 'PAR' in file:\n",
    "        [nii_file, json_file] = convert_par_file(file, tmp_out_dir, tmp_basename)\n",
    "    elif 'nii.gz' in file:\n",
    "        file_ext = \"nii.gz\"\n",
    "        file_ext = f\".{file_ext.lower()}\"\n",
    "    elif 'nii' in file:\n",
    "        file_ext = \"nii.gz\"\n",
    "        file_ext = f\".{file_ext.lower()}\"\n",
    "    else:\n",
    "        file_ext = \"\"\n",
    "        file_ext = f\".{file_ext.lower()}\"\n",
    "        \n",
    "    [nii_file, json_file] = convert_par_file(par_file, tmp_out_dir, tmp_basename)\n",
    "\n",
    "    nii_file = os.path.abspath(nii_file)\n",
    "    json_file = os.path.abspath(json_file)\n",
    "\n",
    "    # Append w to T1/T2 if not already done\n",
    "    if scan in 'T1' or scan in 'T2':\n",
    "        scan = scan + 'w'\n",
    "    else:\n",
    "        scan = scan\n",
    "\n",
    "    # Get Run number\n",
    "    run = get_num_runs(outdir, scan=scan)\n",
    "    run = '{:02}'.format(run)\n",
    "\n",
    "    # Additional sequence/modality parameters\n",
    "#     epi_factor = get_epi_factor(par_file)\n",
    "#     wfs = get_wfs(par_file)\n",
    "#     bval = get_bval(par_file)\n",
    "#     acc = get_acc(par_file)\n",
    "#     mb = get_mb(par_file)\n",
    "#     sct = get_scan_time(par_file)\n",
    "\n",
    "#     # update JSON file with additional parameters\n",
    "#     json_file = update_json(json_file, bval, wfs, epi_factor, acc, mb, sct)\n",
    "\n",
    "    # Create output filenames\n",
    "    out_name = f\"sub-{sub}_ses-{ses}_run-{run}_{scan}\"\n",
    "    out_nii = os.path.join(outdir, out_name + '.nii.gz')\n",
    "    out_json = os.path.join(outdir, out_name + '.json')\n",
    "\n",
    "    os.rename(nii_file, out_nii)\n",
    "    os.rename(json_file, out_json)\n",
    "\n",
    "    # remove temporary directory and leftover files\n",
    "    shutil.rmtree(tmp_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nii_tr(nii_file):\n",
    "    '''\n",
    "    Reads the NifTi file header and returns the repetition time (TR, sec) as a value if it is not zero, otherwise this \n",
    "    function returns the string 'unknown'.\n",
    "    \n",
    "    Arguments:\n",
    "        nii_file (string): NifTi image filename with absolute filepath\n",
    "        \n",
    "    Returns: \n",
    "        tr (float or string): Repetition time (TR, sec), if not zero, otherwise 'unknown' is returned.\n",
    "    '''\n",
    "    \n",
    "    # Load nifti file\n",
    "    img = nib.load(nii_file)\n",
    "    \n",
    "    # Store nifti image TR\n",
    "    tr = float(img.header['pixdim'][4])\n",
    "    \n",
    "    # Check if TR is likely\n",
    "    if tr != 0:\n",
    "        pass\n",
    "    else:\n",
    "        tr = \"unknown\"\n",
    "    \n",
    "    return tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_parts(file):\n",
    "    '''\n",
    "    Divides file with file path into: path, filename, extension.\n",
    "    \n",
    "    Arguments:\n",
    "        file (string): File with absolute filepath\n",
    "        \n",
    "    Returns: \n",
    "        path (string): Path of input file\n",
    "        filename (string): Filename of input file, without the extension\n",
    "        ext (string): Extension of input file\n",
    "    '''\n",
    "    \n",
    "    [path, file_with_ext] = os.path.split(file)\n",
    "    [filename,ext] = os.path.splitext(file_with_ext)\n",
    "    \n",
    "    path = str(path)\n",
    "    filename = str(filename)\n",
    "    ext = str(ext)\n",
    "    \n",
    "    return path,filename,ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gzip_file(file,rm_orig=True):\n",
    "    '''\n",
    "    Gzips file.\n",
    "    \n",
    "    Arguments:\n",
    "        file (string): Input file\n",
    "        rm_orig (boolean): If true (default), removes original file\n",
    "        \n",
    "    Returns: \n",
    "        out_file (string): Gzipped file\n",
    "    '''\n",
    "    \n",
    "    # Define tempory file for I/O buffer stream\n",
    "    tmp_file = file\n",
    "    path,f_name_,ext_ = file_parts(tmp_file)\n",
    "    f_name = f_name_ + ext_ + \".gz\"\n",
    "    out_file = os.path.join(path,f_name)\n",
    "    \n",
    "    # Gzip file\n",
    "    with open(file,\"rb\") as in_file:\n",
    "        data = in_file.read(); in_file.close()\n",
    "        with gzip.GzipFile(out_file,\"wb\") as tmp_out:\n",
    "            tmp_out.write(data)\n",
    "            tmp_out.close()\n",
    "            \n",
    "    if rm_orig:\n",
    "        os.remove(file)\n",
    "            \n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gunzip_file(file,rm_orig=True):\n",
    "    '''\n",
    "    Gunzips file.\n",
    "    \n",
    "    Arguments:\n",
    "        file (string): Input file\n",
    "        rm_orig (boolean): If true (default), removes original file\n",
    "        \n",
    "    Returns: \n",
    "        out_file (string): Gunzipped file\n",
    "    '''\n",
    "    \n",
    "    # Define tempory file for I/O buffer stream\n",
    "    tmp_file = file\n",
    "    path,f_name_,ext_ = file_parts(tmp_file)\n",
    "    f_name = f_name_ # + ext_[:-3]\n",
    "    out_file = os.path.join(path,f_name)\n",
    "    \n",
    "    with gzip.GzipFile(file,\"rb\") as in_file:\n",
    "        data = in_file.read(); in_file.close()\n",
    "        with open(out_file,\"wb\") as tmp_out:\n",
    "            tmp_out.write(data)\n",
    "            tmp_out.close()\n",
    "            \n",
    "    if rm_orig:\n",
    "        os.remove(file)\n",
    "    \n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_json(json_file,dictionary):\n",
    "    '''\n",
    "    Updates JavaScript Object Notation (JSON) file. If the file does not exist, it is created once\n",
    "    this function is invoked.\n",
    "    \n",
    "    Arguments:\n",
    "        json_file (string): Input file\n",
    "        dictionary (dict): Dictionary of key mapped items to write to JSON file\n",
    "        \n",
    "    Returns: \n",
    "        json_file (string): Updated JSON file\n",
    "    '''\n",
    "    \n",
    "    # Check if JSON file exists, if not, then create JSON file\n",
    "    if not os.path.exists(json_file):\n",
    "        with open(json_file,\"w\"): pass\n",
    "        \n",
    "    # Read JSON file\n",
    "    # Try-Except statement has empty exception as JSONDecodeError is not a valid exception to pass, \n",
    "    # thus throwing a name error\n",
    "    try:\n",
    "        with open(json_file) as file:\n",
    "            data_orig = json.load(file)\n",
    "    except:\n",
    "        pass\n",
    "        data_orig = dict()\n",
    "        \n",
    "    # Update original data from JSON file\n",
    "    data_orig.update(dictionary)\n",
    "    \n",
    "    # Write updated JSON file\n",
    "    with open(json_file,\"w\") as file:\n",
    "        json.dump(data_orig,file,indent=4)\n",
    "        \n",
    "    return json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_multi_update(dictionary,**kwargs):\n",
    "    '''\n",
    "    Updates a dictionary multiple times depending on the number key word mapped pairs that are provided and \n",
    "    returns a separate updated dictionary. The dictionary passed as an argument must exist prior to this \n",
    "    function being invoked.\n",
    "    \n",
    "    Example usage:\n",
    "    \n",
    "        new_dict = dict_multi_update(old_dict,\n",
    "                                    Manufacturer=\"Philips\",\n",
    "                                    ManufacturersModelName=\"Ingenia\",\n",
    "                                    MagneticFieldStrength=3,\n",
    "                                    InstitutionName=\"CCHMC\")\n",
    "    \n",
    "    Arguments:\n",
    "        dictionary (dict): Dictionary of key mapped items to write to JSON file\n",
    "        **kwargs (string, key,value pairs): key=value pairs\n",
    "        \n",
    "    Returns: \n",
    "        new_dict (dict): New updated dictionary\n",
    "    '''\n",
    "    \n",
    "    # Create new dictionary\n",
    "    new_dict = dictionary.copy()\n",
    "    \n",
    "    for key,item in kwargs.items():\n",
    "        tmp_dict = {key:item}\n",
    "        new_dict.update(tmp_dict)\n",
    "        \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(dictionary,scan_type=\"\",task=\"\"):\n",
    "    '''\n",
    "    Reads the metadata dictionary and looks for keywords to indicate what metadata should be written to which\n",
    "    dictionary. For example, the keyword 'general' is used to indicate the general information for the imaging\n",
    "    protocol and may contain information such as: field strength, phase encoding direction, institution name, etc.\n",
    "    Additional keywords that are BIDS sub-directories names (e.g. anat, func, dwi) will return an additional\n",
    "    dictionary which contains metadata specific for those modalities. Func also has additional keywords based on\n",
    "    the task specified.\n",
    "    \n",
    "    Arguments:\n",
    "        dictionary (dict): Nest dictionary of key mapped items from the 'read_config' function\n",
    "        scan_type (string): BIDS scan type (e.g. anat, func, dwi, etc., default=\"\")\n",
    "        task (string): Task name to search in the key mapped dictionary\n",
    "        \n",
    "    Returns: \n",
    "        gen_param_dict (dict): General parameters dictionary\n",
    "        scan_param_dict (dict): Scan/modality type parameters dictionary\n",
    "    '''\n",
    "    \n",
    "    # Create empty dictionaries\n",
    "    gen_param_dict = dict()\n",
    "    scan_param_dict = dict()\n",
    "    scan_task_dict = dict()\n",
    "    \n",
    "    # Iterate through, looking for key words (e.g. general and scan_type)\n",
    "    for key,item in dictionary.items():\n",
    "        if key.lower() in 'general':\n",
    "            gen_param_dict = dictionary[key]\n",
    "\n",
    "        if key.lower() in scan_type:\n",
    "            scan_param_dict = dictionary[key]\n",
    "            if task.lower() in scan_param_dict:\n",
    "                for dict_key,dict_item in scan_param_dict.items():\n",
    "                    if task.lower() in dict_key:\n",
    "                        scan_task_dict = scan_param_dict[dict_key]\n",
    "                        \n",
    "        if len(scan_task_dict) != 0:\n",
    "            scan_param_dict = scan_task_dict\n",
    "    \n",
    "    return gen_param_dict, scan_param_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
