{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "import subprocess\n",
    "import pathlib\n",
    "import yaml\n",
    "import nibabel as nib\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import platform\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = \"C:/Users/smart/Desktop/GitProjects/convsauce/ConvertSource/cfg.test.yml\"\n",
    "cfg = \"/Users/brac4g/Desktop/convsauce/ConvertSource/cfg.test.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(config_file, verbose = False):\n",
    "    '''\n",
    "    Reads configuration file and creates a dictionary of search terms for \n",
    "    certain modalities provided that BIDS modalities are used as keys. If\n",
    "    exclusions are provided (via the key 'exclude') then an exclusion list is \n",
    "    created. Otherwise, 'exclusion_list' is returned as an empty list. If \n",
    "    additional settings are specified, they should be done so via the key\n",
    "    'metadata' to enable writing of additional metadata. Otherwise, an \n",
    "    empty dictionary is returned.\n",
    "    \n",
    "    Arguments:\n",
    "        config_file (string): file path to yaml configuration file.\n",
    "        verbose (boolean): Prints additional information to screen.\n",
    "    \n",
    "    Returns: \n",
    "        data_map (dict): Nested dictionary of search terms for BIDS modalities\n",
    "        exclusion_list (list): List of exclusion terms\n",
    "        meta_dict (dict): Nested dictionary of metadata terms to write to JSON file(s)\n",
    "    '''\n",
    "    \n",
    "    with open(config_file) as file:\n",
    "        data_map = yaml.safe_load(file)\n",
    "        if verbose:\n",
    "            print(\"Initialized parameters from configuration file\")\n",
    "        \n",
    "    if any(\"exclude\" in data_map for element in data_map):\n",
    "        if verbose:\n",
    "            print(\"exclusion option implemented\")\n",
    "        exclusion_list = data_map[\"exclude\"]\n",
    "        del data_map[\"exclude\"]\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"exclusion option not implemented\")\n",
    "        exclusion_list = list()\n",
    "        \n",
    "    if any(\"metadata\" in data_map for element in data_map):\n",
    "        if verbose:\n",
    "            print(\"implementing additional settings for metadata\")\n",
    "        meta_dict = data_map[\"metadata\"]\n",
    "        del data_map[\"metadata\"]\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"no metadata settings\")\n",
    "        meta_dict = dict()\n",
    "        \n",
    "    return data_map,exclusion_list,meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized parameters from configuration file\n",
      "exclusion option implemented\n",
      "implementing additional settings for metadata\n"
     ]
    }
   ],
   "source": [
    "search_dict, exclusion_list, param_dict = read_config(cfg,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anat': {'T1w': ['T1', 'T1w', 'TFE'], 'T2w': ['T2', 'T2w', 'TSE']},\n",
       " 'func': {'bold': {'rest': ['rsfMR', 'rest', 'FFE', 'FEEPI'],\n",
       "   'visualstrobe': ['vis', 'visual']}},\n",
       " 'fmap': {'fmap': ['map']},\n",
       " 'swi': {'swi': ['swi']},\n",
       " 'dwi': {'dwi': ['diffusion', 'DTI', 'DWI', '6_DIR']}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SURVEY',\n",
       " 'Reg',\n",
       " 'SHORT',\n",
       " 'LONG',\n",
       " 'MRS',\n",
       " 'PRESS',\n",
       " 'DEFAULT',\n",
       " 'ScreenCapture',\n",
       " 'PD',\n",
       " 'ALL',\n",
       " 'SPECTRO']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclusion_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'common': {'Manufacturer': 'Philips',\n",
       "  'ManufacturersModelName': 'Ingenia',\n",
       "  'MagneticFieldStrength': 3,\n",
       "  'InstitutionName': \"Cincinnati Children's Hospital Medical Center\"},\n",
       " 'func': {'rest': {'ParallelAcquisitionTechnique': 'SENSE',\n",
       "   'PhaseEncodingDirection': 'j',\n",
       "   'MultibandAccelerationFactor': 6,\n",
       "   'TaskName': 'Resting State',\n",
       "   'dir': 'PA',\n",
       "   'NumberOfVolumesDiscardedByScanner': 4},\n",
       "  'visualstrobe': {'PhaseEncodingDirection': 'j',\n",
       "   'TaskName': 'Visual (Strobe) Task',\n",
       "   'NumberOfVolumesDiscardedByScanner': 4}},\n",
       " 'dwi': {'PhaseEncodingDirection': 'j', 'dir': 'PA'},\n",
       " 'fmap': {'Units': 'Hz'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir_par = \"C:/Users/smart/Desktop/GitProjects/convsauce/287H_C10/PAR REC\"\n",
    "data_dir_par = \"/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir_dcm = \"C:/Users/smart/Desktop/GitProjects/convsauce/IRC287H-8/20171003\"\n",
    "data_dir_dcm = \"/Users/brac4g/Desktop/convsauce/IRC287H-8/20171003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir_nii = \"C:/Users/smart/Desktop/GitProjects/convsauce/287H_C10/NIFTI\"\n",
    "data_dir_nii = \"/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dcm_files(dcm_dir):\n",
    "    '''\n",
    "    Creates a file list consisting of the first DICOM file in a parent DICOM directory. \n",
    "    A file list is then returned.\n",
    "    \n",
    "    Arguments:\n",
    "        dcm_dir (string): Absolute path to parent DICOM data directory\n",
    "\n",
    "    Returns: \n",
    "        dcm_files (list): List of DICOM filenames, complete with their absolute paths.\n",
    "    '''\n",
    "    \n",
    "    # Create directory list\n",
    "    dcm_dir = os.path.abspath(dcm_dir)\n",
    "    parent_dcm_dir = os.path.join(dcm_dir,'*')\n",
    "    dcm_dir_list = glob.glob(parent_dcm_dir, recursive=True)\n",
    "\n",
    "    # Initilized dcm_file list\n",
    "    dcm_files = list()\n",
    "    \n",
    "    # Iterate through files in the dicom directory list\n",
    "    for dir_ in dcm_dir_list:\n",
    "        # print(dir_)\n",
    "        for root, dirs, files in os.walk(dir_):\n",
    "            # print(files[0])\n",
    "            tmp_dcm_file = files[0] # only need the first dicom file\n",
    "            tmp_dcm_dir = root\n",
    "            tmp_file = os.path.join(tmp_dcm_dir, tmp_dcm_file)\n",
    "\n",
    "            dcm_files.append(tmp_file)\n",
    "            break\n",
    "\n",
    "    return dcm_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dcm_files(dcm_dir):\n",
    "    '''\n",
    "    Creates a file list consisting of the first DICOM file in a parent DICOM directory. \n",
    "    A file list is then returned.\n",
    "\n",
    "    Arguments:\n",
    "        dcm_dir (string): Absolute path to parent DICOM data directory\n",
    "\n",
    "    Returns: \n",
    "        dcm_files (list): List of DICOM filenames, complete with their absolute paths.\n",
    "    '''\n",
    "\n",
    "    # Create directory list\n",
    "    dcm_dir = os.path.abspath(dcm_dir)\n",
    "    parent_dcm_dir = os.path.join(dcm_dir,'*')\n",
    "    dcm_dir_list = glob.glob(parent_dcm_dir, recursive=True)\n",
    "\n",
    "    # Initilized dcm_file list\n",
    "    dcm_files = list()\n",
    "\n",
    "    # Iterate through files in the dicom directory list\n",
    "    for dir_ in dcm_dir_list:\n",
    "        # print(dir_)\n",
    "        for root, dirs, files in os.walk(dir_):\n",
    "            tmp_dcm_file = files[0] # only need the first dicom file\n",
    "            tmp_dcm_dir = root\n",
    "            tmp_file = os.path.join(tmp_dcm_dir, tmp_dcm_file)\n",
    "\n",
    "            dcm_files.append(tmp_file)\n",
    "            break\n",
    "\n",
    "    return dcm_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_list(data_dir, file_ext=\"\", order=\"size\"):\n",
    "    '''\n",
    "    Creates a file list by globbing a directory for a specific file\n",
    "    extension and sorting by some determined order. A file list is \n",
    "    then returned\n",
    "    \n",
    "    Arguments:\n",
    "        data_dir (string): Absolute path to data directory (must be a directory dump of image data)\n",
    "        file_ext (string): File extension to glob. Built-in options include:\n",
    "            - 'par' or 'PAR': Searches for PAR headers\n",
    "            - 'dcm' or 'DICOM': Searches for DICOM directories, then searches for one file from each DICOM directory\n",
    "            - 'nii', or 'Nifti': Searches for nifti files (including gzipped nifti files)\n",
    "        order (string): Order to sort the list. Valid options are: 'size' and 'time':\n",
    "            - 'size': sorts by file size in ascending order (default)\n",
    "            - 'time': sorts by file modification time in ascending order\n",
    "            - 'none': no sorting is applied and the list is generated as the system finds the files\n",
    "    \n",
    "    Returns: \n",
    "        file_list (list): List of filenames, complete with their absolute paths.\n",
    "    '''\n",
    "    \n",
    "    # Check file extension\n",
    "    if file_ext != \"\":\n",
    "        if file_ext.upper() == \"PAR\" or file_ext.upper() == \"REC\":\n",
    "            file_ext = \"PAR\"\n",
    "            file_ext = f\".{file_ext.upper()}\"\n",
    "        elif file_ext.lower() == \"dcm\" or file_ext.upper() == \"DICOM\":\n",
    "            file_ext = \"dcm\"\n",
    "            file_ext = f\".{file_ext.lower()}\"\n",
    "        elif file_ext.lower() == \"nii\" or file_ext.lower() == \"nifti\":\n",
    "            file_ext = \"nii\"\n",
    "            file_ext = f\".{file_ext.lower()}*\" # Add wildcard for globbling gzipped files\n",
    "        else:\n",
    "            file_ext = f\".{file_ext}\"\n",
    "    \n",
    "    # Check sort order\n",
    "    if order.lower() == \"size\":\n",
    "        order_key = os.path.getsize\n",
    "    elif order.lower() == \"time\":\n",
    "        order_key = os.path.getmtime\n",
    "    elif order.lower() == \"none\":\n",
    "        order_key=None\n",
    "    else:\n",
    "        order_key = os.path.getsize\n",
    "        print(\"Unrecognized keyword option. Using default.\")\n",
    "    \n",
    "    # Create file list\n",
    "    if file_ext == \".dcm\":\n",
    "        file_list = sorted(get_dcm_files(data_dir), key=order_key, reverse=False)\n",
    "    elif file_ext != \".dcm\":\n",
    "        file_names = os.path.join(data_dir, f\"*{file_ext}\")\n",
    "        file_list = sorted(glob.glob(file_names, recursive=True), key=order_key, reverse=False)\n",
    "    \n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_file_list = create_file_list(data_dir=data_dir_par,file_ext=\"par\")\n",
    "dcm_file_list = create_file_list(data_dir=data_dir_dcm,file_ext=\"dcm\")\n",
    "nii_file_list = create_file_list(data_dir=data_dir_nii,file_ext=\"nii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_NEONATAL_SURVEY_2_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_NEONATAL_SURVEY_1_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_NEONATAL_SURVEY_14_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_NEONATAL_SURVEY_16_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_NEONATAL_SURVEY_13_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_NEONATAL_SURVEY_17_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_isoReg_-_WIP_DTI_6DIR_B800_12_6.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_faReg_-_WIP_DTI_6DIR_B800_12_4.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_SAG_4_5.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_SAG_15_5.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_AXIAL__4_4.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_AXIAL__15_4.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_AXIAL__3_3.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_CORONAL__4_3.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_CORONAL__15_3.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_CORONAL__3_2.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_rsfMRI_NR1_MB3_SENSE_1_fat_shift_A_7_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_rsfMRI_NR1_MB3_SENSE_1_fat_shift_P_8_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_Philips_GRE_Map_SENSE_10_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_T2W_TSE_AXIAL_NEONATE_NSA1_4_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_T2W_TSE_AXIAL_NEONATE_NSA1_15_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_SAG_T1W_3D_Y_INNER_TI_1100_3_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_6_DIR_B0_A_TE88_SENSE_NO_MB_NO_4DYN_5_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_SWIp_11_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_dReg_-_WIP_DTI_6DIR_B800_12_3.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_DTI_6DIR_B800_12_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_Reg_-_WIP_DTI_6DIR_B800_12_2.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_6_DIR_B0_A_TE88_6DYN_20_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_36DIR_DTI_B800__P_SENSE_NO__MB_MAX_GRAD_6_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_DTI_B2000_A68_FAT_SHIFT_P_MB2_SENSE_2_TE88_21_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_rsfMRI_400M_MB3_SENSE_1_fat_shift_P_9_1.PAR']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/brac4g/Desktop/convsauce/IRC287H-8/20171003/1101_rsfMRI_MB6_SENSE_1_fat_shift_P_017100310465322437/MR1101027463.dcm',\n",
       " '/Users/brac4g/Desktop/convsauce/IRC287H-8/20171003/0_DEFAULT_PS_SERIES_2017100310463791022/PR0000000001.dcm',\n",
       " '/Users/brac4g/Desktop/convsauce/IRC287H-8/20171003/1701_WM_SV_PRESS_35_017100311174543840/MR1701000001.dcm',\n",
       " '/Users/brac4g/Desktop/convsauce/IRC287H-8/20171003/201_SAG_T1W_3D_Y_INNER_TI_1100_017100310184810020/MR0201000137.dcm',\n",
       " '/Users/brac4g/Desktop/convsauce/IRC287H-8/20171003/303_CORONAL_2017100310262626000/MR0303000091.dcm',\n",
       " '/Users/brac4g/Desktop/convsauce/IRC287H-8/20171003/0_DEFAULT_PS_SERIES_2017100310374358016/PR0000000001.dcm']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcm_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_WIP_Philips_GRE_Map_SENSE_10_1.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_WIP_T2W_TSE_AXIAL_NEONATE_NSA1_15_1.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_WIP_T2W_TSE_AXIAL_NEONATE_NSA1_4_1.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_WIP_SAG_T1W_3D_Y_INNER_TI_1100_3_1.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_CORONAL__15_3.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_AXIAL__15_4.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_SAG_15_5.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_AXIAL__3_3.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_SAG_4_5.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_AXIAL__4_4.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_CORONAL__4_3.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_CORONAL__3_2.nii.gz']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nii_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exclude(file_list, data_dir, exclusion_list = [], verbose = False):\n",
    "    '''\n",
    "    Excludes files from the conversion process by removing filenames\n",
    "    that contain words that match those found in the 'exclusion_list'\n",
    "    from the 'read_config' function - should any files need/want to be \n",
    "    excluded.\n",
    "    \n",
    "    If 'exclusion_list' is empty, then the original 'file_list' is returned.\n",
    "    \n",
    "    Arguments:\n",
    "        file_list (list): List of filenames\n",
    "        data_dir (string): Absolute path to parent directory that contains the image data\n",
    "        exclusion_list (list): List of words to be matched. Filenames that contain these words will be excluded.\n",
    "        verbose (bool): Boolean - True or False.\n",
    "    \n",
    "    Returns: \n",
    "        currated_list (list): Currated list of filenames, with unwanted filenames removed.\n",
    "    '''\n",
    "            \n",
    "    # Check file extension in file list\n",
    "    if 'dcm' in file_list[0]:\n",
    "        file_ext = \"dcm\"\n",
    "        file_ext = f\".{file_ext.lower()}\"\n",
    "    elif 'PAR' in file_list[0]:\n",
    "        file_ext = \"PAR\"\n",
    "        file_ext = f\".{file_ext.upper()}\"\n",
    "    elif 'nii' in file_list[0]:\n",
    "        file_ext = \"nii\"\n",
    "        file_ext = f\".{file_ext.lower()}*\" # Add wildcard for globbling gzipped files\n",
    "    else:\n",
    "        file_ext = \"\"\n",
    "        file_ext = f\".{file_ext.lower()}\"\n",
    "    \n",
    "    # create set of lists\n",
    "    file_set = set(file_list)\n",
    "    \n",
    "    # create empty sets\n",
    "    currated_set = set()\n",
    "    exclusion_set = set()\n",
    "    \n",
    "    if len(exclusion_list) == 0:\n",
    "        currated_set = file_set\n",
    "    else:\n",
    "        for file in exclusion_list:\n",
    "            if file_ext == '.dcm':\n",
    "                dir_ = os.path.join(data_dir, f\"*{file}*\",f\"*{file_ext}\")\n",
    "            else:\n",
    "                dir_ = os.path.join(data_dir, f\"*{file}*{file_ext}\")\n",
    "            f_names = glob.glob(dir_, recursive=True)        \n",
    "            f_names_set = set(f_names)\n",
    "            exclusion_set.update(f_names_set)\n",
    "            \n",
    "        currated_set = file_set.difference(exclusion_set)\n",
    "\n",
    "    currated_list = list(currated_set)\n",
    "    \n",
    "    return currated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "par_file_list_currated = file_exclude(par_file_list,data_dir_par,exclusion_list)\n",
    "dcm_file_list_currated = file_exclude(dcm_file_list,data_dir_dcm,exclusion_list)\n",
    "nii_file_list_currated = file_exclude(nii_file_list,data_dir_nii,exclusion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_AXIAL__4_4.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_T2W_TSE_AXIAL_NEONATE_NSA1_15_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_AXIAL__15_4.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_rsfMRI_NR1_MB3_SENSE_1_fat_shift_P_8_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_6_DIR_B0_A_TE88_SENSE_NO_MB_NO_4DYN_5_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_DTI_6DIR_B800_12_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_SAG_15_5.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_Philips_GRE_Map_SENSE_10_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_T2W_TSE_AXIAL_NEONATE_NSA1_4_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_SAG_T1W_3D_Y_INNER_TI_1100_3_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_rsfMRI_400M_MB3_SENSE_1_fat_shift_P_9_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_CORONAL__4_3.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_rsfMRI_NR1_MB3_SENSE_1_fat_shift_A_7_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_CORONAL__15_3.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_6_DIR_B0_A_TE88_6DYN_20_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_DTI_B2000_A68_FAT_SHIFT_P_MB2_SENSE_2_TE88_21_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_AXIAL__3_3.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_36DIR_DTI_B800__P_SENSE_NO__MB_MAX_GRAD_6_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_SAG_4_5.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_WIP_SWIp_11_1.PAR',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/PAR REC/287H_C10_CORONAL__3_2.PAR']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par_file_list_currated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/brac4g/Desktop/convsauce/IRC287H-8/20171003/1101_rsfMRI_MB6_SENSE_1_fat_shift_P_017100310465322437/MR1101027463.dcm',\n",
       " '/Users/brac4g/Desktop/convsauce/IRC287H-8/20171003/303_CORONAL_2017100310262626000/MR0303000091.dcm',\n",
       " '/Users/brac4g/Desktop/convsauce/IRC287H-8/20171003/201_SAG_T1W_3D_Y_INNER_TI_1100_017100310184810020/MR0201000137.dcm']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcm_file_list_currated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_CORONAL__4_3.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_AXIAL__15_4.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_WIP_SAG_T1W_3D_Y_INNER_TI_1100_3_1.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_WIP_T2W_TSE_AXIAL_NEONATE_NSA1_4_1.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_WIP_T2W_TSE_AXIAL_NEONATE_NSA1_15_1.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_WIP_Philips_GRE_Map_SENSE_10_1.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_CORONAL__15_3.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_SAG_4_5.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_AXIAL__3_3.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_AXIAL__4_4.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_SAG_15_5.nii.gz',\n",
       " '/Users/brac4g/Desktop/convsauce/287H_C10/NIFTI/287H_C10_CORONAL__3_2.nii.gz']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nii_file_list_currated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_in_substr(sub_str_,str_):\n",
    "    '''\n",
    "    DEPRECATED: Should only be used if config_file uses comma separated\n",
    "        lists to denote search terms.\n",
    "    \n",
    "    Searches a (longer) string using a comma separated string \n",
    "    consisting of substrings. Returns 'True' or 'False' if any part\n",
    "    of the substring is found within the larger string.\n",
    "    \n",
    "    Example:\n",
    "        str_in_substr('T1,TFE','sub_T1_image_file') would return True.\n",
    "        str_in_substr('T2,TSE','sub_T1_image_file') would return False.\n",
    "    \n",
    "    Arguments:\n",
    "        sub_str_ (string): Substring used for matching.\n",
    "        str_ (string): Larger string to be searched for matches within substring.\n",
    "    \n",
    "    Returns: \n",
    "        bool_var (bool): Boolean - True or False\n",
    "    '''\n",
    "    \n",
    "    bool_var = False\n",
    "    \n",
    "    for word in sub_str_.split(\",\"):\n",
    "        if any(word in str_ for element in str_):\n",
    "            bool_var = True\n",
    "            \n",
    "    return bool_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_in_substr(list_,str_):\n",
    "    '''\n",
    "    Searches a string using a list that contains substrings. \n",
    "    Returns 'True' or 'False' if any elements of the list are \n",
    "    found within the string.\n",
    "    \n",
    "    Example:\n",
    "        list_in_substr('['T1','TFE']','sub_T1_image_file') would return True.\n",
    "        list_in_substr('['T2','TSE']','sub_T1_image_file') would return False.\n",
    "    \n",
    "    Arguments:\n",
    "        list_ (string): list containing strings used for matching.\n",
    "        str_ (string): Larger string to be searched for matches within substring.\n",
    "    \n",
    "    Returns: \n",
    "        bool_var (bool): Boolean - True or False\n",
    "    '''\n",
    "    \n",
    "    bool_var = False\n",
    "    \n",
    "    for word in list_:\n",
    "        if any(word.lower() in str_.lower() for element in str_.lower()):\n",
    "            bool_var = True\n",
    "            \n",
    "    return bool_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_dcm(dcm_file, verbose=False):\n",
    "    '''\n",
    "    Checks for a valid DICOM file by inspecting the conversion type label in the DICOM file header.\n",
    "    This field should be blank. If this label is populated, then it is likely a secondary capture image \n",
    "    and thus is not likely to contain meaningful image information.\n",
    "    \n",
    "    Arguments:\n",
    "        dcm_file (string): DICOM filename with absolute filepath\n",
    "        verbose (boolean): Enable verbosity\n",
    "    \n",
    "    Returns: \n",
    "        is_valid (boolean): True if DICOM file is not a secondary capture (or does not have text in the conversion type label field)\n",
    "    '''\n",
    "    \n",
    "    # Read DICOM file header\n",
    "    ds = pydicom.dcmread(dcm_file)\n",
    "    \n",
    "    # Invalid files include secondary image captures, and are not suitable for \n",
    "    # nifti conversion as they are often not converted and cause problems.\n",
    "    # This string should be empty. If it is populated, then its likely a secondary capture.\n",
    "    conv_type = ds.ConversionType\n",
    "    \n",
    "    if conv_type in '':\n",
    "        is_valid = True\n",
    "    else:\n",
    "        is_valid = False\n",
    "        if verbose:\n",
    "            print(f\"Please check Conversion Type (0008, 0064) in dicom header. The presented DICOM file is not a valid file: {dcm_file}.\")\n",
    "    \n",
    "    return is_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scan_tech(search_dict, file, json_file=\"\"):\n",
    "    '''\n",
    "    Searches DICOM or PAR file header for scan technique/MR modality used in accordance with the search terms provided\n",
    "    by the nested dictionary.\n",
    "    \n",
    "    Note: This function is still undergoing active development.\n",
    "    \n",
    "    Arguments:\n",
    "        search_dict (dict): Nested dictionary from the 'read_config' function\n",
    "        dcm_file (string): Source image filename with absolute filepath\n",
    "    \n",
    "    Returns: \n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    # Check file extension in file\n",
    "    # Perform Scanning Techniqe Search\n",
    "    if '.dcm' in file.lower():\n",
    "        get_dcm_scan_tech(search_dict,file)\n",
    "    elif '.PAR' in file.upper():\n",
    "        get_par_scan_tech(search_dict,file)\n",
    "    else:\n",
    "        print(\"unknown modality\")\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dcm_scan_tech(dcm_file, search_dict, keep_unknown=True, verbose=False):\n",
    "    '''\n",
    "    Searches DICOM file header for scan technique/MR modality used in accordance with the search terms provided by the\n",
    "    nested dictionary. The DICOM header field searched is a Philips DICOM private tag (2001,1020) [Scanning Technique \n",
    "    Description MR]. In the case that field does not match, is empty, or does not exist, then more common DICOM tags\n",
    "    are searched - and they include: Series Description, Protocol Name, and Image Type.\n",
    "    \n",
    "    Note: This function is still undergoing active development.\n",
    "    \n",
    "    Arguments:\n",
    "        search_dict (dict): Nested dictionary from the 'read_config' function\n",
    "        dcm_file (string): DICOM filename with absolute filepath\n",
    "    \n",
    "    Returns: \n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    mod_found = False\n",
    "    \n",
    "    # Load DICOM data and read header\n",
    "    ds = pydicom.dcmread(dcm_file)\n",
    "    \n",
    "    # Search DICOM header for Scan Technique used\n",
    "    dcm_scan_tech_str = str(ds[0x2001,0x1020])\n",
    "    \n",
    "    for key,item in search_dict.items():\n",
    "        for dict_key,dict_item in search_dict[key].items():\n",
    "            if isinstance(dict_item,list):\n",
    "                if list_in_substr(dict_item,dcm_scan_tech_str):\n",
    "                    mod_found = True\n",
    "                    if verbose:\n",
    "                        print(f\"{key} - {dict_key}: {dict_item}\")\n",
    "                    scan_type = key\n",
    "                    scan = dict_key\n",
    "                    if scan_type.lower() == 'dwi':\n",
    "                        data_to_bids_dwi(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_dwi=\"\",ses=\"\",scan_type=scan_type)\n",
    "                    elif scan_type.lower() == 'fmap':\n",
    "                        data_to_bids_fmap(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_fmap=\"\",ses=\"\",scan_type=scan_type)\n",
    "                    else:\n",
    "                        data_to_bids_anat(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_anat=\"\",ses=\"\",scan_type=scan_type)\n",
    "                    if mod_found:\n",
    "                        break\n",
    "            elif isinstance(dict_item,dict):\n",
    "                tmp_dict = search_dict[key]\n",
    "                for d_key,d_item in tmp_dict[dict_key].items():\n",
    "                    if list_in_substr(d_item,dcm_scan_tech_str):\n",
    "                        mod_found = True\n",
    "                        if verbose:\n",
    "                            print(f\"{key} - {dict_key} - {d_key}: {d_item}\")\n",
    "                        scan_type = key\n",
    "                        scan = dict_key\n",
    "                        task = d_key\n",
    "                        if scan_type.lower() == 'func':\n",
    "                            data_to_bids_func(bids_out_dir,file,sub,scan,task=\"\",meta_dict_com,meta_dict_func=\"\",ses=\"\",scan_type=scan_type)\n",
    "                        elif scan_type.lower() == 'dwi':\n",
    "                            data_to_bids_dwi(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_dwi=\"\",ses=\"\",scan_type=scan_type)\n",
    "                        elif scan_type.lower() == 'fmap':\n",
    "                            data_to_bids_fmap(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_fmap=\"\",ses=\"\",scan_type=scan_type)\n",
    "                        else:\n",
    "                            data_to_bids_anat(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_anat=\"\",ses=\"\",scan_type=scan_type)\n",
    "                        if mod_found:\n",
    "                            break\n",
    "                            \n",
    "        if mod_found:\n",
    "            break\n",
    "    \n",
    "    # Secondary look in the case Private Field (2001, 1020) [Scanning Technique Description MR] is empty\n",
    "    if not mod_found:\n",
    "        # Define list of DICOM header fields\n",
    "        dcm_fields = ['SeriesDescription', 'ImageType', 'ProtocolName']\n",
    "        \n",
    "        for dcm_field in dcm_fields:\n",
    "            dcm_scan_tech_str = str(eval(f\"ds.{dcm_field}\")) # This makes me dangerously uncomfortable\n",
    "            \n",
    "            for key,item in search_dict.items():\n",
    "                for dict_key,dict_item in search_dict[key].items():\n",
    "                    if isinstance(dict_item,list):\n",
    "                        if list_in_substr(dict_item,dcm_scan_tech_str):\n",
    "                            mod_found = True\n",
    "                            if verbose:\n",
    "                                print(f\"{key} - {dict_key}: {dict_item}\")\n",
    "                            scan_type = key\n",
    "                            scan = dict_key\n",
    "                            if scan_type.lower() == 'dwi':\n",
    "                                data_to_bids_dwi(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_dwi=\"\",ses=\"\",scan_type=scan_type)\n",
    "                            elif scan_type.lower() == 'fmap':\n",
    "                                data_to_bids_fmap(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_fmap=\"\",ses=\"\",scan_type=scan_type)\n",
    "                            else:\n",
    "                                data_to_bids_anat(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_anat=\"\",ses=\"\",scan_type=scan_type)\n",
    "                            if mod_found:\n",
    "                                break\n",
    "                    elif isinstance(dict_item,dict):\n",
    "                        tmp_dict = search_dict[key]\n",
    "                        for d_key,d_item in tmp_dict[dict_key].items():\n",
    "                            if list_in_substr(d_item,dcm_scan_tech_str):\n",
    "                                mod_found = True\n",
    "                                if verbose:\n",
    "                                    print(f\"{key} - {dict_key} - {d_key}: {d_item}\")\n",
    "                                scan_type = key\n",
    "                                scan = dict_key\n",
    "                                task = d_key\n",
    "                                if scan_type.lower() == 'func':\n",
    "                                    data_to_bids_func(bids_out_dir,file,sub,scan,task=\"\",meta_dict_com,meta_dict_func=\"\",ses=\"\",scan_type=scan_type)\n",
    "                                elif scan_type.lower() == 'dwi':\n",
    "                                    data_to_bids_dwi(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_dwi=\"\",ses=\"\",scan_type=scan_type)\n",
    "                                elif scan_type.lower() == 'fmap':\n",
    "                                    data_to_bids_fmap(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_fmap=\"\",ses=\"\",scan_type=scan_type)\n",
    "                                else:\n",
    "                                    data_to_bids_anat(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_anat=\"\",ses=\"\",scan_type=scan_type)\n",
    "                                if mod_found:\n",
    "                                    break\n",
    "\n",
    "            if mod_found:\n",
    "                break\n",
    "                \n",
    "    if not mod_found:\n",
    "        if verbose:\n",
    "            print(\"unknown modality\")\n",
    "        if keep_unknown:\n",
    "            scan_type = 'unknown_modality'\n",
    "            scan = 'unknown'\n",
    "            data_to_bids_anat(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_anat=\"\",ses=\"\",scan_type=scan_type)\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_par_scan_tech(par_file, search_dict, keep_unknown=True, verbose=False):\n",
    "    '''\n",
    "    Searches PAR file header for scan technique/MR modality used in accordance with the search terms provided by the\n",
    "    nested dictionary. A regular expression (regEx) search string is defined and searched for conventional PAR headers.\n",
    "    \n",
    "    Note: This function is still undergoing active development.\n",
    "    \n",
    "    Arguments:\n",
    "        search_dict (dict): Nested dictionary from the 'read_config' function\n",
    "        par_file (string): PAR filename with absolute filepath\n",
    "    \n",
    "    Returns: \n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    mod_found = False\n",
    "    \n",
    "    # Define regEx search string\n",
    "    regexp = re.compile(r'.    Technique                          :  .*', re.M | re.I)\n",
    "    \n",
    "    # Open and search PAR header file\n",
    "    with open(par_file) as f:\n",
    "        for line in f:\n",
    "            match_ = regexp.match(line)\n",
    "            if match_:\n",
    "                par_scan_tech_str = match_.group()\n",
    "    \n",
    "    # Search Scan Technique with search terms\n",
    "    for key,item in search_dict.items():\n",
    "        for dict_key,dict_item in search_dict[key].items():\n",
    "            if isinstance(dict_item,list):\n",
    "                if list_in_substr(dict_item,par_scan_tech_str):\n",
    "                    mod_found = True\n",
    "                    if verbose:\n",
    "                        print(f\"{key} - {dict_key}: {dict_item}\")\n",
    "                    scan_type = key\n",
    "                    scan = dict_key\n",
    "                    if scan_type.lower() == 'dwi':\n",
    "                        data_to_bids_dwi(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_dwi=\"\",ses=\"\",scan_type=scan_type)\n",
    "                    elif scan_type.lower() == 'fmap':\n",
    "                        data_to_bids_fmap(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_fmap=\"\",ses=\"\",scan_type=scan_type)\n",
    "                    else:\n",
    "                        data_to_bids_anat(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_anat=\"\",ses=\"\",scan_type=scan_type)\n",
    "                    if mod_found:\n",
    "                        break\n",
    "            elif isinstance(dict_item,dict):\n",
    "                tmp_dict = search_dict[key]\n",
    "                for d_key,d_item in tmp_dict[dict_key].items():\n",
    "                    if list_in_substr(d_item,par_scan_tech_str):\n",
    "                        mod_found = True\n",
    "                        if verbose:\n",
    "                            print(f\"{key} - {dict_key} - {d_key}: {d_item}\")\n",
    "                        scan_type = key\n",
    "                        scan = dict_key\n",
    "                        task = d_key\n",
    "                        if scan_type.lower() == 'func':\n",
    "                            data_to_bids_func(bids_out_dir,file,sub,scan,task=\"\",meta_dict_com,meta_dict_func=\"\",ses=\"\",scan_type=scan_type)\n",
    "                        elif scan_type.lower() == 'dwi':\n",
    "                            data_to_bids_dwi(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_dwi=\"\",ses=\"\",scan_type=scan_type)\n",
    "                        elif scan_type.lower() == 'fmap':\n",
    "                            data_to_bids_fmap(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_fmap=\"\",ses=\"\",scan_type=scan_type)\n",
    "                        else:\n",
    "                            data_to_bids_anat(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_anat=\"\",ses=\"\",scan_type=scan_type)\n",
    "                        if mod_found:\n",
    "                            break\n",
    "                            \n",
    "        if mod_found:\n",
    "            break\n",
    "            \n",
    "    if not mod_found:\n",
    "        if verbose:\n",
    "            print(\"unknown modality\")\n",
    "        if keep_unknown:\n",
    "            scan_type = 'unknown_modality'\n",
    "            scan = 'unknown'\n",
    "            data_to_bids_anat(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_anat=\"\",ses=\"\",scan_type=scan_type)\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_modality(file, search_dict, verbose=False):\n",
    "    '''\n",
    "    Converts an image file and extracts information from the filename (such as the modality). \n",
    "    \n",
    "    Note: This function is still undergoing active development.\n",
    "    Note: Add support for extra dictionaries\n",
    "    \n",
    "    Arguments:\n",
    "        search_dict (dict): Nested dictionary from the 'read_config' function\n",
    "        file (string): Filename with absolute filepath\n",
    "        verbose (boolean): Enable verbosity\n",
    "    \n",
    "    Returns: \n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    mod_found = False\n",
    "    \n",
    "    # Check file type\n",
    "    if 'nii' in file:\n",
    "        file_ext = \"nii\"\n",
    "        file_ext = f\".{file_ext.lower()}\"\n",
    "    elif 'dcm' in file:\n",
    "        file_ext = \"dcm\"\n",
    "        file_ext = f\".{file_ext.lower()}\"\n",
    "        if not is_valid_dcm(file,verbose):\n",
    "            sys.exit(f\"Invalid DICOM file. Please check {file}\")\n",
    "    \n",
    "    for key,item in search_dict.items():\n",
    "        for dict_key,dict_item in search_dict[key].items():\n",
    "            if isinstance(dict_item,list):\n",
    "                if list_in_substr(dict_item,file):\n",
    "                    mod_found = True\n",
    "                    if verbose:\n",
    "                        print(f\"{key} - {dict_key}: {dict_item}\")\n",
    "                    scan_type = key\n",
    "                    scan = dict_key\n",
    "                    if scan_type.lower() == 'dwi':\n",
    "                        data_to_bids_dwi(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_dwi=\"\",ses=\"\",scan_type=scan_type)\n",
    "                    elif scan_type.lower() == 'fmap':\n",
    "                        data_to_bids_fmap(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_fmap=\"\",ses=\"\",scan_type=scan_type)\n",
    "                    else:\n",
    "                        data_to_bids_anat(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_anat=\"\",ses=\"\",scan_type=scan_type)\n",
    "                    if mod_found:\n",
    "                        break\n",
    "            elif isinstance(dict_item,dict):\n",
    "                tmp_dict = search_dict[key]\n",
    "                for d_key,d_item in tmp_dict[dict_key].items():\n",
    "                    if list_in_substr(d_item,file):\n",
    "                        mod_found = True\n",
    "                        if verbose:\n",
    "                            print(f\"{key} - {dict_key} - {d_key}: {d_item}\")\n",
    "                        scan_type = key\n",
    "                        scan = dict_key\n",
    "                        task = d_key\n",
    "                        if scan_type.lower() == 'func':\n",
    "                            data_to_bids_func(bids_out_dir,file,sub,scan,task=\"\",meta_dict_com,meta_dict_func=\"\",ses=\"\",scan_type=scan_type)\n",
    "                        elif scan_type.lower() == 'dwi':\n",
    "                            data_to_bids_dwi(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_dwi=\"\",ses=\"\",scan_type=scan_type)\n",
    "                        elif scan_type.lower() == 'fmap':\n",
    "                            data_to_bids_fmap(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_fmap=\"\",ses=\"\",scan_type=scan_type)\n",
    "                        else:\n",
    "                            data_to_bids_anat(bids_out_dir,file,sub,scan,meta_dict_com,meta_dict_anat=\"\",ses=\"\",scan_type=scan_type)\n",
    "                        if mod_found:\n",
    "                            break\n",
    "                        \n",
    "    if not mod_found:\n",
    "        get_scan_tech(search_dict,file)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_convert(file_list, dictionary, verbose=False):\n",
    "    '''\n",
    "    Batch conversion function for image files. \n",
    "    \n",
    "    Note: This function is still undergoing active development.\n",
    "    \n",
    "    Arguments:\n",
    "        file_list (list): List of filenames with absolute filepaths\n",
    "        dictionary (dict): Nested dictionary from the 'read_config' function\n",
    "        verbose (boolean): Enable verbosity\n",
    "    \n",
    "    Returns: \n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    for file in file_list:\n",
    "        try:\n",
    "            convert_modality(dictionary,file,verbose)\n",
    "        except SystemExit:\n",
    "            pass\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swi - swi: ['swi']\n",
      "func - bold - rest: ['rsfMR', 'rest', 'FFE', 'FEEPI']\n",
      "anat - T2w: ['T2', 'T2w', 'TSE']\n",
      "anat - T2w: ['T2', 'T2w', 'TSE']\n",
      "anat - T2w: ['T2', 'T2w', 'TSE']\n",
      "anat - T2w: ['T2', 'T2w', 'TSE']\n",
      "dwi - dwi: ['diffusion', 'DTI', 'DWI', '6_DIR']\n",
      "anat - T2w: ['T2', 'T2w', 'TSE']\n",
      "func - bold - rest: ['rsfMR', 'rest', 'FFE', 'FEEPI']\n",
      "anat - T2w: ['T2', 'T2w', 'TSE']\n",
      "dwi - dwi: ['diffusion', 'DTI', 'DWI', '6_DIR']\n",
      "dwi - dwi: ['diffusion', 'DTI', 'DWI', '6_DIR']\n",
      "dwi - dwi: ['diffusion', 'DTI', 'DWI', '6_DIR']\n",
      "anat - T2w: ['T2', 'T2w', 'TSE']\n",
      "anat - T1w: ['T1', 'T1w', 'TFE']\n",
      "fmap - fmap: ['map']\n",
      "dwi - dwi: ['diffusion', 'DTI', 'DWI', '6_DIR']\n",
      "anat - T2w: ['T2', 'T2w', 'TSE']\n",
      "anat - T1w: ['T1', 'T1w', 'TFE']\n",
      "func - bold - rest: ['rsfMR', 'rest', 'FFE', 'FEEPI']\n",
      "anat - T1w: ['T1', 'T1w', 'TFE']\n"
     ]
    }
   ],
   "source": [
    "batch_convert(par_file_list_currated,search_dict,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anat - T1w: ['T1', 'T1w', 'TFE']\n",
      "func - bold - rest: ['rsfMR', 'rest', 'FFE', 'FEEPI']\n",
      "anat - T2w: ['T2', 'T2w', 'TSE']\n"
     ]
    }
   ],
   "source": [
    "batch_convert(dcm_file_list_currated,search_dict,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TaskName` JSON file appending funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_name = \"\"\n",
    "task_name = \"visualstrobe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_name is: visualstrobe\n"
     ]
    }
   ],
   "source": [
    "if task_name == \"\":\n",
    "    print(\"task_name is empty\")\n",
    "else:\n",
    "    print(f\"task_name is: {task_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nii_file = \"c:/Users/smart/Desktop/GitProjects/convsauce/BIDS/rawdata/sub-C10/ses-001/func/sub-C10_ses-001_task-rest_acq-PA_run-01_bold.nii.gz\"\n",
    "nii_file = \"/Users/brac4g/Desktop/convsauce/BIDS/rawdata/sub-C10/ses-001/func/sub-C10_ses-001_task-rest_acq-PA_run-01_bold.nii.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(nii_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = nib.load(nii_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nibabel.nifti1.Nifti1Image at 0x7fa7683c7048>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 45, 400)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.header.get_data_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img.header.get_data_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = img.header.get_data_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 45, 400)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = dims[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_frames(nii_file):\n",
    "    '''\n",
    "    working doc-string\n",
    "    '''\n",
    "    \n",
    "    img = nib.load(nii_file)\n",
    "    dims = img.header.get_data_shape()\n",
    "    num_frames = dims[3]\n",
    "    \n",
    "    return num_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_frames(nii_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `NifTi` File Conversion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_anat(file,work_dir,work_name):\n",
    "    '''\n",
    "    Converts raw anatomical (and functional) MR images to NifTi file format, with a BIDS JSON sidecar.\n",
    "    Returns a NifTi file and a JSON sidecar (file) by globbing an isolated directory.\n",
    "    \n",
    "    Arguments:\n",
    "        file (string): Absolute filepath to raw image data\n",
    "        work_dir (string): Working directory\n",
    "        work_name (string): Output file name\n",
    "        \n",
    "    Returns:\n",
    "        nii_file (string): Absolute file path to NifTi image\n",
    "        json_file (string): Absolute file path to JSON sidecar\n",
    "    '''\n",
    "    \n",
    "    # Convert (anatomical) iamge data\n",
    "    convert_image_data(file, work_name, work_dir)\n",
    "    \n",
    "    # Get files\n",
    "    dir_path = os.path.join(work_dir, basename)\n",
    "    nii_file = glob.glob(dir_path + '*.nii*')\n",
    "    json_file = glob.glob(dir_path + '*.json')\n",
    "    \n",
    "    # Convert lists to strings\n",
    "    nii_file = ''.join(nii_file)\n",
    "    json_file = ''.join(json_file)\n",
    "    \n",
    "    return nii_file, json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dwi(file,work_dir,work_name):\n",
    "    '''\n",
    "    Converts raw diffusion weigthed MR images to NifTi file format, with a BIDS JSON sidecar.\n",
    "    Returns a NifTi file, JSON sidecar (file), and (FSL-style) bval and bvec files by globbing \n",
    "    an isolated directory.\n",
    "    \n",
    "    Arguments:\n",
    "        file (string): Absolute filepath to raw image data\n",
    "        work_dir (string): Working directory\n",
    "        work_name (string): Output file name\n",
    "        \n",
    "    Returns:\n",
    "        nii_file (string): Absolute file path to NifTi image\n",
    "        json_file (string): Absolute file path to JSON sidecar\n",
    "        bval (string): Absolute file path to bval file\n",
    "        bvec (string): Absolute file path to bvec file\n",
    "    '''\n",
    "    \n",
    "    # Convert diffusion iamge data\n",
    "    convert_image_data(file, work_name, work_dir)\n",
    "    \n",
    "    # Get files\n",
    "    dir_path = os.path.join(out_dir, basename)\n",
    "    nii_file = glob.glob(dir_path + '*.nii*')\n",
    "    json_file = glob.glob(dir_path + '*.json')\n",
    "    bval = glob.glob(dir_path + '*.bval*')\n",
    "    bvec = glob.glob(dir_path + '*.bvec*')\n",
    "    \n",
    "    # Convert lists to strings\n",
    "    nii_file = ''.join(nii_file)\n",
    "    json_file = ''.join(json_file)\n",
    "    bval = ''.join(bval)\n",
    "    bvec = ''.join(bvec)\n",
    "    \n",
    "    return nii_file, json_file, bval, bvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_fmap(file,work_dir,work_name):\n",
    "    '''\n",
    "    Converts raw precomputed fieldmap MR images to NifTi file format, with a BIDS JSON sidecar.\n",
    "    Returns two NifTi files, and their corresponding JSON sidecars (files), by globbing an isolated directory.\n",
    "    \n",
    "    N.B.: This function is mainly designed to handle fieldmap data case 3 from bids-specifications document. Furhter support for \n",
    "    the additional cases requires test/validation data. \n",
    "    BIDS-specifications document located here: \n",
    "    https://github.com/bids-standard/bids-specification/blob/master/src/04-modality-specific-files/01-magnetic-resonance-imaging-data.md\n",
    "    \n",
    "    Arguments:\n",
    "        file (string): Absolute filepath to raw image data\n",
    "        work_dir (string): Working directory\n",
    "        work_name (string): Output file name\n",
    "        \n",
    "    Returns:\n",
    "        nii_fmap (string): Absolute file path to NifTi image fieldmap\n",
    "        json_fmap (string): Absolute file path to corresponding JSON sidecar\n",
    "        nii_mag (string): Absolute file path to NifTi magnitude image\n",
    "        json_mag (string): Absolute file path to corresponding JSON sidecar\n",
    "    '''\n",
    "    \n",
    "    # Convert diffusion iamge data\n",
    "    convert_image_data(file, work_name, work_dir)\n",
    "    \n",
    "    # Get files\n",
    "    dir_path = os.path.join(out_dir, basename)\n",
    "    nii_fmap = glob.glob(dir_path + '*real*.nii*')\n",
    "    json_fmap = glob.glob(dir_path + '*real*.json')\n",
    "    nii_mag = glob.glob(dir_path + '.nii*')\n",
    "    json_mag = glob.glob(dir_path + '.json')\n",
    "    \n",
    "    # Convert lists to strings\n",
    "    nii_fmap = ''.join(nii_real)\n",
    "    json_fmap = ''.join(json_real)\n",
    "    nii_mag = ''.join(nii_mag)\n",
    "    json_mag = ''.join(json_mag)\n",
    "\n",
    "    return nii_fmap, json_fmap, nii_mag, json_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_bids_anat(bids_out_dir, file, sub, scan, meta_dict_com=dict(), meta_dict_anat=dict(), ses=1, scan_type='anat'):\n",
    "    '''\n",
    "    Renames converted NifTi-2 files to conform with the BIDS naming convension (in the case of anatomical files).\n",
    "    This function accepts any image file (DICOM, PAR REC, and NifTi-2). If the image file is a raw data file (e.g. DICOM, PAR REC)\n",
    "    it is converted to NifTi first, then renamed. The output BIDS directory need not exist at runtime.\n",
    "    \n",
    "    Arguments:\n",
    "        bids_out_dir (string): Path to output BIDS directory. \n",
    "        file (string): Filepath to image file.\n",
    "        sub (int or string): Subject ID\n",
    "        scan (string): Modality (e.g. T1w, T2w, or SWI)\n",
    "        meta_dict_com (dict): Metadata dictionary for common image metadata\n",
    "        meta_dict_anat (dict): Metadata dictionary for common anatomical image specific metadata\n",
    "        ses (int or string): Session ID\n",
    "        scan_type (string): BIDS sub-directory scan type. Valid options include, but are not limited to: anat (default), func, fmap, dwi, etc.\n",
    "        \n",
    "    Returns:\n",
    "        out_nii (string): Absolute filepath to gzipped output NifTi-2 file\n",
    "        out_json (string): Absolute filepath to corresponding JSON file\n",
    "    '''\n",
    "\n",
    "    # Create Output Directory Variables\n",
    "    # Zeropad subject ID if possible\n",
    "    try:\n",
    "        ses = '{:03}'.format(int(ses))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    # Zeropad session ID if possible\n",
    "    try:\n",
    "        ses = '{:03}'.format(int(ses))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    out_dir = os.path.join(bids_out_dir, f\"sub-{sub}\", f\"ses-{ses}\", f\"{scan_type}\")\n",
    "\n",
    "    # Make output directory\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    # Get absolute filepaths\n",
    "    bids_out_dir = os.path.abspath(bids_out_dir)\n",
    "    out_dir = os.path.abspath(out_dir)\n",
    "    \n",
    "    # Create temporary output names/directories\n",
    "    n = 10000 # maximum N for random number generator\n",
    "    tmp_out_dir = os.path.join(out_dir, f\"sub-{sub}\", 'tmp_dir' + str(random.randint(0, n)))\n",
    "    tmp_basename = 'tmp_basename' + str(random.randint(0, n))\n",
    "    \n",
    "    if not os.path.exists(tmp_out_dir):\n",
    "        os.makedirs(tmp_out_dir)\n",
    "\n",
    "    # Convert image file\n",
    "    # Check file extension in file\n",
    "    if '.nii.gz' in file:\n",
    "        nii_file = cp_file(file, tmp_out_dir, tmp_basename)\n",
    "        [path,filename] = file_parts(file)\n",
    "        json_file = os.path.join(path,filename + '.json')\n",
    "        try:\n",
    "            json_file = cp_file(json_file, tmp_out_dir, tmp_basename)\n",
    "        except FileNotFoundError:\n",
    "            json_file = os.path.join(tmp_out_dir, tmp_basename + '.json')\n",
    "            pass\n",
    "    elif '.nii' in file:\n",
    "        nii_file = cp_file(file, tmp_out_dir, tmp_basename)\n",
    "        nii_file = gzip_file(nii_file)\n",
    "        json_file = os.path.join(path,filename + '.json')\n",
    "        try:\n",
    "            json_file = cp_file(json_file, tmp_out_dir, tmp_basename)\n",
    "        except FileNotFoundError:\n",
    "            json_file = os.path.join(tmp_out_dir, tmp_basename + '.json')\n",
    "            pass\n",
    "    elif '.dcm' in file or '.PAR' in file:\n",
    "        [nii_file, json_file] = convert_anat(file,tmp_out_dir,tmp_basename)\n",
    "    else:\n",
    "        [nii_file, json_file] = convert_anat(file,tmp_out_dir,tmp_basename)\n",
    "    \n",
    "    # Get additional sequence/modality parameters\n",
    "    if os.path.exists(json_file):\n",
    "        meta_dict_params = get_data_params(file, json_file)\n",
    "    else:\n",
    "        tmp_json = \"\"\n",
    "        meta_dict_params = get_data_params(file, tmp_json)\n",
    "    \n",
    "    # Update JSON file\n",
    "    info = dict()\n",
    "    info = dict_multi_update(info,**meta_dict_com)\n",
    "    info = dict_multi_update(info,**meta_dict_params)\n",
    "    info = dict_multi_update(info,**meta_dict_anat)\n",
    "    \n",
    "    json_file = update_json(json_file,info)\n",
    "    \n",
    "    nii_file = os.path.abspath(nii_file)\n",
    "    json_file = os.path.abspath(json_file)\n",
    "\n",
    "    # Append w to T1/T2 if not already done\n",
    "    if scan in 'T1' or scan in 'T2':\n",
    "        scan = scan + 'w'\n",
    "\n",
    "    # Query dictionary for acquisition/naming keys\n",
    "    try:\n",
    "        acq = info['acq']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        ce = info['ce']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        rec = info['rec']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    # Create output filename\n",
    "    out_name = f\"sub-{sub}\" + f\"_ses-{sub}\"\n",
    "    name_run_dict = dict()\n",
    "\n",
    "    if acq:\n",
    "        out_name = out_name + f\"_acq-{acq}\"\n",
    "        tmp_dict = {\"acq\":f\"{acq}\"}\n",
    "        name_run_dict.update(tmp_dict)\n",
    "\n",
    "    if ce:\n",
    "        out_name = out_name + f\"_ce-{ce}\"\n",
    "        tmp_dict = {\"ce\":f\"{ce}\"}\n",
    "        name_run_dict.update(tmp_dict)\n",
    "\n",
    "    if rec:\n",
    "        out_name = out_name + f\"_rec-{rec}\"\n",
    "        tmp_dict = {\"rec\":f\"{rec}\"}\n",
    "        name_run_dict.update(tmp_dict)\n",
    "        \n",
    "    # Get Run number\n",
    "    run = get_num_runs(outdir, scan=scan, **name_run_dict)\n",
    "    run = '{:02}'.format(run)\n",
    "\n",
    "    if run:\n",
    "        out_name = out_name + f\"_run-{run}\"\n",
    "\n",
    "    out_name = out_name + f\"_{scan}\"\n",
    "\n",
    "\n",
    "    out_nii = os.path.join(out_dir, out_name + '.nii.gz')\n",
    "    out_json = os.path.join(out_dir, out_name + '.json')\n",
    "\n",
    "    os.rename(nii_file, out_nii)\n",
    "    os.rename(json_file, out_json)\n",
    "\n",
    "    # remove temporary directory and leftover files\n",
    "    shutil.rmtree(tmp_out_dir)\n",
    "    \n",
    "    return out_nii,out_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_bids_func(bids_out_dir, file, sub, scan, task = 'rest', meta_dict_com=dict(), meta_dict_func=dict(), ses=1, scan_type='func'):\n",
    "    '''\n",
    "    Renames converted NifTi-2 files to conform with the BIDS naming convension (in the case of functional files).\n",
    "    This function accepts any image file (DICOM, PAR REC, and NifTi-2). If the image file is a raw data file (e.g. DICOM, PAR REC)\n",
    "    it is converted to NifTi first, then renamed. The output BIDS directory need not exist at runtime.\n",
    "    \n",
    "    Arguments:\n",
    "        bids_out_dir (string): Path to output BIDS directory. \n",
    "        file (string): Filepath to image file.\n",
    "        sub (int or string): Subject ID\n",
    "        scan (string): Modality (e.g. bold or cbv)\n",
    "        task (string): Task for the fMR image data\n",
    "        meta_dict_com (dict): Metadata dictionary for common image metadata\n",
    "        meta_dict_func (dict): Metadata dictionary for common functional image specific metadata\n",
    "        ses (int or string): Session ID\n",
    "        scan_type (string): BIDS sub-directory scan type. Valid options include, but are not limited to: anat, func (default), fmap, dwi, etc.\n",
    "        \n",
    "    Returns:\n",
    "        out_nii (string): Absolute filepath to gzipped output 4D NifTi-2 file\n",
    "        out_json (string): Absolute filepath to corresponding JSON file\n",
    "    '''\n",
    "\n",
    "    # Create Output Directory Variables\n",
    "    # Zeropad subject ID if possible\n",
    "    try:\n",
    "        ses = '{:03}'.format(int(ses))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    # Zeropad session ID if possible\n",
    "    try:\n",
    "        ses = '{:03}'.format(int(ses))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    out_dir = os.path.join(bids_out_dir, f\"sub-{sub}\", f\"ses-{ses}\", f\"{scan_type}\")\n",
    "\n",
    "    # Make output directory\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    # Get absolute filepaths\n",
    "    bids_out_dir = os.path.abspath(bids_out_dir)\n",
    "    out_dir = os.path.abspath(out_dir)\n",
    "    \n",
    "    # Create temporary output names/directories\n",
    "    n = 10000 # maximum N for random number generator\n",
    "    tmp_out_dir = os.path.join(out_dir, f\"sub-{sub}\", 'tmp_dir' + str(random.randint(0, n)))\n",
    "    tmp_basename = 'tmp_basename' + str(random.randint(0, n))\n",
    "    \n",
    "    if not os.path.exists(tmp_out_dir):\n",
    "        os.makedirs(tmp_out_dir)\n",
    "\n",
    "    # Convert image file\n",
    "    # Check file extension in file\n",
    "    if '.nii.gz' in file:\n",
    "        nii_file = cp_file(file, tmp_out_dir, tmp_basename)\n",
    "        [path,filename] = file_parts(file)\n",
    "        json_file = os.path.join(path,filename + '.json')\n",
    "        try:\n",
    "            json_file = cp_file(json_file, tmp_out_dir, tmp_basename)\n",
    "        except FileNotFoundError:\n",
    "            json_file = os.path.join(tmp_out_dir, tmp_basename + '.json')\n",
    "            pass\n",
    "    elif '.nii' in file:\n",
    "        nii_file = cp_file(file, tmp_out_dir, tmp_basename)\n",
    "        nii_file = gzip_file(nii_file)\n",
    "        json_file = os.path.join(path,filename + '.json')\n",
    "        try:\n",
    "            json_file = cp_file(json_file, tmp_out_dir, tmp_basename)\n",
    "        except FileNotFoundError:\n",
    "            json_file = os.path.join(tmp_out_dir, tmp_basename + '.json')\n",
    "            pass\n",
    "    elif '.dcm' in file or '.PAR' in file:\n",
    "        [nii_file, json_file] = convert_anat(file,tmp_out_dir,tmp_basename)\n",
    "    else:\n",
    "        [nii_file, json_file] = convert_anat(file,tmp_out_dir,tmp_basename)\n",
    "    \n",
    "    # Get additional sequence/modality parameters\n",
    "    if os.path.exists(json_file):\n",
    "        meta_dict_params = get_data_params(file, json_file)\n",
    "    else:\n",
    "        tmp_json = \"\"\n",
    "        meta_dict_params = get_data_params(file, tmp_json)\n",
    "    \n",
    "    # Update JSON file\n",
    "    info = dict()\n",
    "    info = dict_multi_update(info,**meta_dict_com)\n",
    "    info = dict_multi_update(info,**meta_dict_params)\n",
    "    info = dict_multi_update(info,**meta_dict_func)\n",
    "    \n",
    "    json_file = update_json(json_file,info)\n",
    "    \n",
    "    nii_file = os.path.abspath(nii_file)\n",
    "    json_file = os.path.abspath(json_file)\n",
    "    \n",
    "    # Decide if file is 4D timeseries or single-band reference\n",
    "    num_frames = get_num_frames(nii_file)\n",
    "    if num_frames == 1:\n",
    "        scan = 'sbref'\n",
    "\n",
    "    # Query dictionary for acquisition/naming keys\n",
    "    try:\n",
    "        acq = info['acq']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        ce = info['ce']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        direction = info['dir']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        rec = info['rec']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        echo = info['echo']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    # Create output filename    \n",
    "    out_name = f\"sub-{sub}\" + f\"_ses-{ses}\" + f\"_task-{task}\"\n",
    "    \n",
    "    name_run_dict = dict()\n",
    "    tmp_dict = {\"task\":f\"{task}\"}\n",
    "    name_run_dict.update(tmp_dict)\n",
    "\n",
    "    if acq:\n",
    "        out_name = out_name + f\"_acq-{acq}\"\n",
    "        tmp_dict = {\"acq\":f\"{acq}\"}\n",
    "        name_run_dict.update(tmp_dict)\n",
    "\n",
    "    if ce:\n",
    "        out_name = out_name + f\"_ce-{ce}\"\n",
    "        tmp_dict = {\"ce\":f\"{ce}\"}\n",
    "        name_run_dict.update(tmp_dict)\n",
    "\n",
    "    if direction:\n",
    "        out_name = out_name + f\"_dir-{direction}\"\n",
    "        tmp_dict = {\"dirs\":f\"{direction}\"}\n",
    "        name_run_dict.update(tmp_dict)\n",
    "\n",
    "    if rec:\n",
    "        out_name = out_name + f\"_rec-{rec}\"\n",
    "        tmp_dict = {\"rec\":f\"{rec}\"}\n",
    "        name_run_dict.update(tmp_dict)\n",
    "        \n",
    "    if echo:\n",
    "        tmp_dict = {\"echo\":f\"{echo}\"}\n",
    "        name_run_dict.update(tmp_dict)\n",
    "        \n",
    "    # Get Run number\n",
    "    run = get_num_runs(outdir, scan=scan, **name_run_dict)\n",
    "    run = '{:02}'.format(run)\n",
    "\n",
    "    if run:\n",
    "        out_name = out_name + f\"_run-{run}\"\n",
    "\n",
    "    if echo:\n",
    "        out_name = out_name + f\"_echo-{echo}\"\n",
    "\n",
    "    out_name = out_name + f\"_{scan}\"\n",
    "\n",
    "\n",
    "    out_nii = os.path.join(out_dir, out_name + '.nii.gz')\n",
    "    out_json = os.path.join(out_dir, out_name + '.json')\n",
    "\n",
    "    os.rename(nii_file, out_nii)\n",
    "    os.rename(json_file, out_json)\n",
    "\n",
    "    # remove temporary directory and leftover files\n",
    "    shutil.rmtree(tmp_out_dir)\n",
    "    \n",
    "    return out_nii,out_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_bids_fmap(bids_out_dir, file, sub, scan='magnitude', meta_dict_com=dict(), meta_dict_fmap=dict(), ses=1, scan_type='fmap'):\n",
    "    '''\n",
    "    Renames converted NifTi-2 files to conform with the BIDS naming convension (in the case of fieldmap files).\n",
    "    This function accepts any image file (DICOM, PAR REC, and NifTi-2). If the image file is a raw data file (e.g. DICOM, PAR REC)\n",
    "    it is converted to NifTi first, then renamed. The output BIDS directory need not exist at runtime.\n",
    "    \n",
    "    N.B.: This function is mainly designed to handle fieldmap data case 3 from bids-specifications document. Furhter support for \n",
    "    the additional cases requires test/validation data. \n",
    "    BIDS-specifications document located here: \n",
    "    https://github.com/bids-standard/bids-specification/blob/master/src/04-modality-specific-files/01-magnetic-resonance-imaging-data.md\n",
    "    \n",
    "    Arguments:\n",
    "        bids_out_dir (string): Path to output BIDS directory. \n",
    "        file (string): Filepath to image file.\n",
    "        sub (int or string): Subject ID\n",
    "        scan (string): Modality (e.g. fieldmap, magnitude, or phasediff)\n",
    "        meta_dict_com (dict): Metadata dictionary for common image metadata\n",
    "        meta_dict_fmap (dict): Metadata dictionary for common fieldmap image specific metadata\n",
    "        ses (int or string): Session ID\n",
    "        scan_type (string): BIDS sub-directory scan type. Valid options include, but are not limited to: anat, func, fmap (default), dwi, etc.\n",
    "        \n",
    "    Returns:\n",
    "        out_nii_fmap (string): Absolute filepath to gzipped output NifTi-2 fieldmap image file\n",
    "        out_nii_mag (string): Absolute filepath to gzipped output NifTi-2 magnitude image file\n",
    "        out_json_fmap (string): Absolute filepath to correspond fieldmap image JSON sidecare\n",
    "        out_json_mag (string): Absolute filepath to correspond magnitude image JSON sidecare\n",
    "    '''\n",
    "\n",
    "    # Create Output Directory Variables\n",
    "    # Zeropad subject ID if possible\n",
    "    try:\n",
    "        ses = '{:03}'.format(int(ses))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    # Zeropad session ID if possible\n",
    "    try:\n",
    "        ses = '{:03}'.format(int(ses))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    out_dir = os.path.join(bids_out_dir, f\"sub-{sub}\", f\"ses-{ses}\", f\"{scan_type}\")\n",
    "\n",
    "    # Make output directory\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    # Get absolute filepaths\n",
    "    bids_out_dir = os.path.abspath(bids_out_dir)\n",
    "    out_dir = os.path.abspath(out_dir)\n",
    "    \n",
    "    # Create temporary output names/directories\n",
    "    n = 10000 # maximum N for random number generator\n",
    "    tmp_out_dir = os.path.join(out_dir, f\"sub-{sub}\", 'tmp_dir' + str(random.randint(0, n)))\n",
    "    tmp_basename = 'tmp_basename' + str(random.randint(0, n))\n",
    "    \n",
    "    if not os.path.exists(tmp_out_dir):\n",
    "        os.makedirs(tmp_out_dir)\n",
    "\n",
    "    # Convert image file\n",
    "    # Check file extension in file\n",
    "    if '.nii.gz' in file:\n",
    "        nii_file = cp_file(file, tmp_out_dir, tmp_basename)\n",
    "        [path,filename] = file_parts(file)\n",
    "        json_file = os.path.join(path,filename + '.json')\n",
    "        try:\n",
    "            json_file = cp_file(json_file, tmp_out_dir, tmp_basename)\n",
    "        except FileNotFoundError:\n",
    "            json_file = os.path.join(tmp_out_dir, tmp_basename + '.json')\n",
    "            pass\n",
    "    elif '.nii' in file:\n",
    "        nii_file = cp_file(file, tmp_out_dir, tmp_basename)\n",
    "        nii_file = gzip_file(nii_file)\n",
    "        json_file = os.path.join(path,filename + '.json')\n",
    "        try:\n",
    "            json_file = cp_file(json_file, tmp_out_dir, tmp_basename)\n",
    "        except FileNotFoundError:\n",
    "            json_file = os.path.join(tmp_out_dir, tmp_basename + '.json')\n",
    "            pass\n",
    "    elif '.dcm' in file or '.PAR' in file:\n",
    "        [nii_fmap, json_fmap, nii_mag, json_mag] = convert_fmap(file,tmp_out_dir,tmp_basename)\n",
    "    else:\n",
    "        [nii_fmap, json_fmap, nii_mag, json_mag] = convert_fmap(file,tmp_out_dir,tmp_basename)\n",
    "    \n",
    "    # Get additional sequence/modality parameters\n",
    "    if os.path.exists(json_fmap):\n",
    "        meta_dict_params = get_data_params(file, json_fmap)\n",
    "    else:\n",
    "        tmp_json = \"\"\n",
    "        meta_dict_params = get_data_params(file, tmp_json)\n",
    "    \n",
    "    # Update JSON file\n",
    "    info = dict()\n",
    "    info = dict_multi_update(info,**meta_dict_com)\n",
    "    info = dict_multi_update(info,**meta_dict_params)\n",
    "    info = dict_multi_update(info,**meta_dict_fmap)\n",
    "    \n",
    "    json_fmap = update_json(json_fmap,info)\n",
    "    json_mag = update_json(json_mag,info)\n",
    "    \n",
    "    nii_fmap = os.path.abspath(nii_fmap)\n",
    "    nii_mag = os.path.abspath(nii_mag)\n",
    "    \n",
    "    json_fmap = os.path.abspath(json_fmap)\n",
    "    json_mag = os.path.abspath(json_mag)\n",
    "\n",
    "    # Query dictionary for acquisition/naming keys\n",
    "    try:\n",
    "        acq = info['acq']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    # Create output filename    \n",
    "    out_name = f\"sub-{sub}\" + f\"_ses-{ses}\"\n",
    "    name_run_dict = dict()\n",
    "\n",
    "    if acq:\n",
    "        out_name = out_name + f\"_acq-{acq}\"\n",
    "        tmp_dict = {\"acq\":f\"{acq}\"}\n",
    "        name_run_dict.update(tmp_dict)\n",
    "        \n",
    "    # Get Run number\n",
    "    run = get_num_runs(outdir, scan=scan, **name_run_dict)\n",
    "    run = '{:02}'.format(run)\n",
    "\n",
    "    if run:\n",
    "        out_name = out_name + f\"_run-{run}\"\n",
    "\n",
    "    out_name = out_name + f\"_{scan}\"\n",
    "    \n",
    "    out_nii_fmap = os.path.join(out_dir, out_name + '_fieldmap' + '.nii.gz')\n",
    "    out_nii_mag = os.path.join(out_dir, out_name + '_magnitude' + '.nii.gz')\n",
    "    \n",
    "    out_json_fmap = os.path.join(out_dir, out_name + '_fieldmap' + '.json')\n",
    "    out_json_mag = os.path.join(out_dir, out_name + '_magnitude' + '.json')\n",
    "\n",
    "    os.rename(nii_fmap, out_nii_fmap)\n",
    "    os.rename(nii_mag, out_nii_mag)\n",
    "    \n",
    "    os.rename(json_fmap, out_json_fmap)\n",
    "    os.rename(json_mag, out_json_mag)\n",
    "\n",
    "    # Remove temporary directory and leftover files\n",
    "    shutil.rmtree(tmp_out_dir)\n",
    "    \n",
    "    return out_nii_fmap, out_nii_mag, out_json_fmap, out_json_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_bids_dwi(bids_out_dir, file, sub, scan='dwi', meta_dict_com=dict(), meta_dict_dwi=dict(), ses=1, scan_type='dwi'):\n",
    "    '''\n",
    "    Renames converted NifTi-2 files to conform with the BIDS naming convension (in the case of diffuion image files).\n",
    "    This function accepts any image file (DICOM, PAR REC, and NifTi-2). If the image file is a raw data file (e.g. DICOM, PAR REC)\n",
    "    it is converted to NifTi first, then renamed. The output BIDS directory need not exist at runtime. If the original\n",
    "    data format is NifTi, bval and bvec files will be copied over should they exist, otherwise, they will not be\n",
    "    generated.\n",
    "    \n",
    "    Arguments:\n",
    "        bids_out_dir (string): Path to output BIDS directory. \n",
    "        file (string): Filepath to image file.\n",
    "        sub (int or string): Subject ID\n",
    "        scan (string): Modality (e.g. dwi, dki, etc)\n",
    "        meta_dict_com (dict): Metadata dictionary for common image metadata\n",
    "        meta_dict_dwi (dict): Metadata dictionary for common diffusion image specific metadata\n",
    "        ses (int or string): Session ID\n",
    "        scan_type (string): BIDS sub-directory scan type. Valid options include, but are not limited to: anat, func, fmap, dwi (default), etc.\n",
    "        \n",
    "    Returns:\n",
    "        out_nii (string): Absolute filepath to gzipped output diffusion weighted NifTi-2 file\n",
    "        out_json (string): Absolute filepath to corresponding JSON file\n",
    "        out_bval (string): Absolute filepath to corresponding b-values file\n",
    "        out_bvec (string): Absolute filepath to corresponding b-vectors file\n",
    "    '''\n",
    "\n",
    "    # Create Output Directory Variables\n",
    "    # Zeropad subject ID if possible\n",
    "    try:\n",
    "        ses = '{:03}'.format(int(ses))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    # Zeropad session ID if possible\n",
    "    try:\n",
    "        ses = '{:03}'.format(int(ses))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    out_dir = os.path.join(bids_out_dir, f\"sub-{sub}\", f\"ses-{ses}\", f\"{scan_type}\")\n",
    "\n",
    "    # Make output directory\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    # Get absolute filepaths\n",
    "    bids_out_dir = os.path.abspath(bids_out_dir)\n",
    "    out_dir = os.path.abspath(out_dir)\n",
    "    \n",
    "    # Create temporary output names/directories\n",
    "    n = 10000 # maximum N for random number generator\n",
    "    tmp_out_dir = os.path.join(out_dir, f\"sub-{sub}\", 'tmp_dir' + str(random.randint(0, n)))\n",
    "    tmp_basename = 'tmp_basename' + str(random.randint(0, n))\n",
    "    \n",
    "    if not os.path.exists(tmp_out_dir):\n",
    "        os.makedirs(tmp_out_dir)\n",
    "\n",
    "    # Convert image file\n",
    "    # Check file extension in file\n",
    "    if '.nii.gz' in file:\n",
    "        nii_file = cp_file(file, tmp_out_dir, tmp_basename)\n",
    "        [path,filename] = file_parts(file)\n",
    "        json_file = os.path.join(path,filename + '.json')\n",
    "        bval = os.path.join(path,filename + '.bval*')\n",
    "        bvec = os.path.join(path,filename + '.bvec*')\n",
    "        try:\n",
    "            json_file = cp_file(json_file, tmp_out_dir, tmp_basename)\n",
    "            bval = cp_file(bval, tmp_out_dir, tmp_basename)\n",
    "            bvec = cp_file(bvec, tmp_out_dir, tmp_basename)\n",
    "        except FileNotFoundError:\n",
    "            json_file = os.path.join(tmp_out_dir, tmp_basename + '.json')\n",
    "            bval = os.path.join(tmp_out_dir, tmp_basename + '.bval')\n",
    "            bvec = os.path.join(tmp_out_dir, tmp_basename + '.bvec')\n",
    "            pass\n",
    "    elif '.nii' in file:\n",
    "        nii_file = cp_file(file, tmp_out_dir, tmp_basename)\n",
    "        nii_file = gzip_file(nii_file)\n",
    "        json_file = os.path.join(path,filename + '.json')\n",
    "        bval = os.path.join(path,filename + '.bval*')\n",
    "        bvec = os.path.join(path,filename + '.bvec*')\n",
    "        try:\n",
    "            json_file = cp_file(json_file, tmp_out_dir, tmp_basename)\n",
    "            bval = cp_file(bval, tmp_out_dir, tmp_basename)\n",
    "            bvec = cp_file(bvec, tmp_out_dir, tmp_basename)\n",
    "        except FileNotFoundError:\n",
    "            json_file = os.path.join(tmp_out_dir, tmp_basename + '.json')\n",
    "            bval = os.path.join(tmp_out_dir, tmp_basename + '.bval')\n",
    "            bvec = os.path.join(tmp_out_dir, tmp_basename + '.bvec')\n",
    "            pass\n",
    "    elif '.dcm' in file or '.PAR' in file:\n",
    "        [nii_file, json_file, bval, bvec] = convert_dwi(file,tmp_out_dir,tmp_basename)\n",
    "    else:\n",
    "        [nii_file, json_file, bval, bvec] = convert_dwi(file,tmp_out_dir,tmp_basename)\n",
    "    \n",
    "    # Get additional sequence/modality parameters\n",
    "    if os.path.exists(json_file):\n",
    "        meta_dict_params = get_data_params(file, json_file, bval)\n",
    "    else:\n",
    "        tmp_json = \"\"\n",
    "        meta_dict_params = get_data_params(file, tmp_json, bval)\n",
    "    \n",
    "    # Update JSON file\n",
    "    info = dict()\n",
    "    info = dict_multi_update(info,**meta_dict_com)\n",
    "    info = dict_multi_update(info,**meta_dict_params)\n",
    "    info = dict_multi_update(info,**meta_dict_dwi)\n",
    "    \n",
    "    json_file = update_json(json_file,info)\n",
    "    \n",
    "    nii_file = os.path.abspath(nii_file)\n",
    "    json_file = os.path.abspath(json_file)\n",
    "    \n",
    "    bval = os.path.abspath(bval)\n",
    "    bvec = os.path.abspath(bvec)\n",
    "    \n",
    "    # Decide if file is 4D timeseries or single-band reference\n",
    "    num_frames = get_num_frames(nii_file)\n",
    "    if num_frames == 1:\n",
    "        scan = 'sbref'\n",
    "\n",
    "    # Query dictionary for acquisition/naming keys\n",
    "    try:\n",
    "        acq = info['acq']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        direction = info['dir']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    # Non-standard acquisition/naming keys\n",
    "    # Used in order to differentiate between DWI scans for multiple bvalues\n",
    "    try:\n",
    "        bvals = info['bval']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        echo_time = info['EchoTime']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    # Create output filename    \n",
    "    out_name = f\"sub-{sub}\" + f\"_ses-{ses}\"\n",
    "    name_run_dict = dict()\n",
    "    \n",
    "    if bvals:\n",
    "        vals = \"\"\n",
    "        for val in bvals:\n",
    "            vals = vals + 'b' + val\n",
    "    \n",
    "    if bvals and acq and echo_time:\n",
    "        out_name = out_name + f\"_acq-{acq}{vals}TE{echo_time}\"\n",
    "        tmp_dict = {\"acq\":f\"{acq}{vals}TE{echo_time}\"}\n",
    "    elif bvals and acq:\n",
    "        out_name = out_name + f\"_acq-{acq}{vals}\"\n",
    "        tmp_dict = {\"acq\":f\"{acq}{vals}\"}\n",
    "    elif bvals and echo_time:\n",
    "        out_name = out_name + f\"_acq-{vals}TE{echo_time}\"\n",
    "        tmp_dict = {\"acq\":f\"{vals}TE{echo_time}\"}\n",
    "    elif acq and echo_time:\n",
    "        out_name = out_name + f\"_acq-{acq}TE{echo_time}\"\n",
    "        tmp_dict = {\"acq\":f\"{acq}TE{echo_time}\"}\n",
    "    elif acq:\n",
    "        out_name = out_name + f\"_acq-{acq}\"\n",
    "        tmp_dict = {\"acq\":f\"{acq}\"}\n",
    "    elif bvals:\n",
    "        out_name = out_name + f\"_acq-{vals}\"\n",
    "        tmp_dict = {\"acq\":f\"{vals}\"}\n",
    "    elif echo_time:\n",
    "        out_name = out_name + f\"_acq-TE{echo_time}\"\n",
    "        tmp_dict = {\"acq\":f\"TE{echo_time}\"}\n",
    "    else:\n",
    "        tmp_dict = dict()\n",
    "        \n",
    "    name_run_dict.update(tmp_dict)\n",
    "\n",
    "    if direction:\n",
    "        out_name = out_name + f\"_dir-{direction}\"\n",
    "        tmp_dict = {\"dirs\":f\"{direction}\"}\n",
    "        name_run_dict.update(tmp_dict)\n",
    "        \n",
    "    # Get Run number\n",
    "    run = get_num_runs(outdir, scan=scan, **name_run_dict)\n",
    "    run = '{:02}'.format(run)\n",
    "\n",
    "    if run:\n",
    "        out_name = out_name + f\"_run-{run}\"\n",
    "\n",
    "    out_name = out_name + f\"_{scan}\"\n",
    "\n",
    "\n",
    "    out_nii = os.path.join(out_dir, out_name + '.nii.gz')\n",
    "    out_json = os.path.join(out_dir, out_name + '.json')\n",
    "    \n",
    "    out_bval = os.path.join(out_dir, out_name + '.bval')\n",
    "    out_bvec = os.path.join(out_dir, out_name + '.bvec')\n",
    "\n",
    "    os.rename(nii_file, out_nii)\n",
    "    os.rename(json_file, out_json)\n",
    "    \n",
    "    if bval:\n",
    "        os.rename(bval,out_bval)\n",
    "        \n",
    "    if bvec:\n",
    "        os.rename(bvec,out_bvec)\n",
    "\n",
    "    # remove temporary directory and leftover files\n",
    "    shutil.rmtree(tmp_out_dir)\n",
    "    \n",
    "    return out_nii,out_json,out_bval,out_bvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_params(file,json_file=\"\", bval_file=\"\"):\n",
    "    '''\n",
    "    Creates a dictionary of key mapped parameter items that are often not written to the BIDS JSON sidecar\n",
    "    when converting Philips DICOM and PAR REC files.\n",
    "    \n",
    "    Arguments:\n",
    "        file (string): Absolute filepath to raw image data file (DICOM or PAR REC)\n",
    "        json_file (string, optional): Corresponding JSON sidecare file\n",
    "        bval_file (string, optional): Corresponding bval file for DWI acquisitions\n",
    "    \n",
    "    Returns:\n",
    "        info (dict): Dictionary of key mapped items/values\n",
    "    '''\n",
    "    \n",
    "    # Create empty dictionary\n",
    "    tmp_dict = dict()\n",
    "    \n",
    "    # Check and write bvalue(s) to file\n",
    "    if bval_file:\n",
    "        bval_list = get_bvals(bval_file)\n",
    "        tmp_dict.update({\"bval\":bval_list})\n",
    "    \n",
    "    # Check file type\n",
    "    if '.dcm' in file.lower():\n",
    "        red_fact = get_red_fact(file)\n",
    "        mb = get_mb(file)\n",
    "        scan_time = get_scan_time(file)\n",
    "        [eff_echo_sp, tot_read_time]  = calc_read_time(file,json_file)\n",
    "        source_format = \"DICOM\"\n",
    "        tmp_dict.update({\"ParallelReductionFactorInPlane\": red_fact,\n",
    "                         \"MultibandAccelerationFactor\": mb,\n",
    "                         \"EffectiveEchoSpacing\": eff_echo_sp,\n",
    "                         \"TotalReadoutTime\": tot_read_time,\n",
    "                         \"AcquisitionDuration\": scan_time,\n",
    "                         \"SourceDataFormat\": source_format})\n",
    "    elif 'PAR' in file.upper():\n",
    "        wfs = get_wfs(file)\n",
    "        red_fact = get_red_fact(file)\n",
    "        mb = get_mb(file)\n",
    "        scan_time = get_scan_time(file)\n",
    "        etl = get_etl(file)\n",
    "        [eff_echo_sp, tot_read_time]  = calc_read_time(file,json_file)\n",
    "        source_format = \"PAR REC\"\n",
    "        tmp_dict.update({\"WaterFatShift\": wfs,\n",
    "                         \"ParallelAcquisitionTechnique\": 'SENSE',\n",
    "                         \"ParallelReductionFactorInPlane\": red_fact,\n",
    "                         \"MultibandAccelerationFactor\": mb,\n",
    "                         \"EffectiveEchoSpacing\": eff_echo_sp,\n",
    "                         \"TotalReadoutTime\": tot_read_time,\n",
    "                         \"AcquisitionDuration\": scan_time,\n",
    "                         \"EchoTrainLength\": etl,\n",
    "                         \"SourceDataFormat\": source_format})\n",
    "    elif 'nii' in file.lower():\n",
    "        tr = get_nii_tr(file)\n",
    "        source_format = \"NIFTI\"\n",
    "        tmp_dict.update({\"RepetitionTime\": tr,\n",
    "                         \"SourceDataFormat\": source_format})\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    info = dict()\n",
    "    info.update(tmp_dict)\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DICOM` Parameter Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_red_fact(dcm_file):\n",
    "    '''\n",
    "    Extracts parallel reduction factor in-plane value (GRAPPA/SENSE) from file description in the DICOM \n",
    "    header for MR scanners. This reduction factor is assumed to be 1 if a value cannot be found from witin\n",
    "    the DICOM header.\n",
    "    \n",
    "    Arguments:\n",
    "        dcm_file (string): Absolute filepath to DICOM file\n",
    "        \n",
    "    Returns:\n",
    "        red_fact (float): parallel reduction factor in-plane value (e.g. SENSE factor)\n",
    "    '''\n",
    "\n",
    "    # Load dicom data\n",
    "    ds = pydicom.dcmread(dcm_file)\n",
    "    red_fact = \"\"\n",
    "    \n",
    "    # Get Info\n",
    "    try:\n",
    "        red_fact = ds[0x0018,0x9069]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    # Get image descriptor\n",
    "    if not red_fact:\n",
    "        line = ds.SeriesDescription\n",
    "        match = re.search(r'SENSE .*?([0-9.-]+)', line, re.M | re.I)\n",
    "        if match:\n",
    "            red_fact = match.group(1)\n",
    "            red_fact = float(red_fact)\n",
    "        else:\n",
    "            red_fact = float(1)\n",
    "\n",
    "    return red_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mb(dcm_file):\n",
    "    '''\n",
    "    Extracts multi-band acceleration factor from file description in the DICOM header for philips MR scanners.\n",
    "    \n",
    "    N.B.: This is done via a regEx search as no DICOM tag stores this information explicitly.\n",
    "    \n",
    "    Arguments:\n",
    "        dcm_file (string): Absolute filepath to DICOM file\n",
    "        \n",
    "    Returns:\n",
    "        mb (int): multi-band acceleration factor\n",
    "    '''\n",
    "\n",
    "    # Initialize mb to 1\n",
    "    mb = 1\n",
    "\n",
    "    # Load dicom data\n",
    "    ds = pydicom.dcmread(dcm_file)\n",
    "\n",
    "    # Get image descriptor\n",
    "    line = ds.SeriesDescription\n",
    "    match = re.search(r'MB.*?([0-9.-]+)', line, re.M | re.I)\n",
    "    if match:\n",
    "        mb = match.group(1)\n",
    "        mb = int(mb)\n",
    "\n",
    "    return mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scan_time(dcm_file):\n",
    "    '''\n",
    "    Gets the acquisition duration (scan time, in s) from the DICOM header.\n",
    "    \n",
    "    Arguments:\n",
    "        dcm_file (string): Absolute filepath to DICOM file\n",
    "        \n",
    "    Returns:\n",
    "        scan_time (float): acquisition duration (scan time, in s)\n",
    "    '''\n",
    "\n",
    "    # Load data\n",
    "    ds = pydicom.dcmread(dcm_file)\n",
    "\n",
    "    # Gets scan time\n",
    "    try:\n",
    "        scan_time = ds.AcquisitionDuration\n",
    "    except AttributeError:\n",
    "        pass\n",
    "        scan_time = 'unknown'\n",
    "\n",
    "    return scan_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_echo(json_file):\n",
    "    '''\n",
    "    Reads the echo time (TE) from the NifTi JSON sidecar and returns it.\n",
    "\n",
    "    Arguments:\n",
    "        json_file (string): Absolute path to JSON sidecar\n",
    "\n",
    "    Returns:\n",
    "        echo (float): Returns the echo time as a float.\n",
    "    '''\n",
    "\n",
    "    with open(json_file, \"r\") as read_file:\n",
    "        data = json.load(read_file)\n",
    "\n",
    "    echo = data.get(\"EchoTime\")\n",
    "\n",
    "    return echo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `PAR REC` Parameter Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_etl(par_file):\n",
    "    '''\n",
    "    Gets EPI factor (Echo Train Length) from Philips' PAR Header.\n",
    "    \n",
    "    N.B.: This is done via a regEx search as the PAR header is not assumed to change significantly between scanners.\n",
    "    \n",
    "    Arguments:\n",
    "        par_file (string): Absolute filepath to PAR header file\n",
    "        \n",
    "    Returns:\n",
    "        etl (float): Echo Train Length\n",
    "    '''\n",
    "    regexp = re.compile(r'.    EPI factor        <0,1=no EPI>     :   .*?([0-9.-]+)')  # Search string for RegEx\n",
    "    with open(par_file) as f:\n",
    "        for line in f:\n",
    "            match = regexp.match(line)\n",
    "            if match:\n",
    "                etl = match.group(1)\n",
    "                etl = int(etl)\n",
    "    return etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wfs(par_file):\n",
    "    '''\n",
    "    Gets Water Fat Shift from Philips' PAR Header.\n",
    "    \n",
    "    N.B.: This is done via a regEx search as the PAR header is not assumed to change significantly between scanners.\n",
    "    \n",
    "    Arguments:\n",
    "        par_file (string): Absolute filepath to PAR header file\n",
    "        \n",
    "    Returns:\n",
    "        wfs (float): Water Fat Shift\n",
    "    '''\n",
    "    regexp = re.compile(\n",
    "        r'.    Water Fat shift \\[pixels\\]           :   .*?([0-9.-]+)')  # Search string for RegEx, escape the []\n",
    "    with open(par_file) as f:\n",
    "        for line in f:\n",
    "            match = regexp.match(line)\n",
    "            if match:\n",
    "                wfs = match.group(1)\n",
    "                wfs = float(wfs)\n",
    "    return wfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_red_fact(par_file):\n",
    "    '''\n",
    "    Extracts parallel reduction factor in-plane value (SENSE) from the file description in the PAR REC header \n",
    "    for Philips MR scanners. This reduction factor is assumed to be 1 if a value cannot be found from witin\n",
    "    the PAR REC header.\n",
    "    \n",
    "    N.B.: This is done via a regEx search as the PAR header is not assumed to change significantly between scanners.\n",
    "    \n",
    "    Arguments:\n",
    "        par_file (string): Absolute filepath to PAR header file\n",
    "        \n",
    "    Returns:\n",
    "        red_fact (float): parallel reduction factor in-plane value (e.g. SENSE factor)\n",
    "    '''\n",
    "    \n",
    "    # Read file\n",
    "    red_fact = \"\"\n",
    "    regexp = re.compile(r' SENSE *?([0-9.-]+)')\n",
    "    with open(par_file) as f:\n",
    "        for line in f:\n",
    "            match = regexp.search(line)\n",
    "            if match:\n",
    "                red_fact = match.group(1)\n",
    "                red_fact = float(red_fact)\n",
    "            else:\n",
    "                red_fact = float(1)\n",
    "\n",
    "    return red_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mb(par_file):\n",
    "    '''\n",
    "    Extracts multi-band acceleration factor from from Philips' PAR Header.\n",
    "    \n",
    "    N.B.: This is done via a regEx search as the PAR header does not normally store this value.\n",
    "    \n",
    "    Arguments:\n",
    "        par_file (string): Absolute filepath to PAR header file\n",
    "        \n",
    "    Returns:\n",
    "        mb (int): multi-band acceleration factor\n",
    "    '''\n",
    "\n",
    "    # Initialize mb to 1\n",
    "    mb = 1\n",
    "    \n",
    "    regexp = re.compile(r' MB *?([0-9.-]+)')\n",
    "    with open(par_file) as f:\n",
    "        for line in f:\n",
    "            match = regexp.search(line)\n",
    "            if match:\n",
    "                mb = match.group(1)\n",
    "                mb = int(mb)\n",
    "\n",
    "    return mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scan_time(par_file):\n",
    "    '''\n",
    "    Gets the acquisition duration (scan time, in s) from the PAR header.\n",
    "    \n",
    "    N.B.: This is done via a regEx search as the PAR header is not assumed to change significantly between scanners.\n",
    "    \n",
    "    Arguments:\n",
    "        par_file (string): Absolute filepath to PAR header file\n",
    "        \n",
    "    Returns:\n",
    "        scan_time (float or string): Acquisition duration (scan time, in s). If not in header, return is a string 'unknown'\n",
    "    '''\n",
    "    \n",
    "    scan_time = 'unknown'\n",
    "    \n",
    "    regexp = re.compile(\n",
    "        r'.    Scan Duration \\[sec\\]                :   .*?([0-9.-]+)')  # Search string for RegEx, escape the []\n",
    "    with open(par_file) as f:\n",
    "        for line in f:\n",
    "            match = regexp.match(line)\n",
    "            if match:\n",
    "                scan_time = match.group(1)\n",
    "                scan_time = float(scan_time)\n",
    "    return scan_time\n",
    "\n",
    "    return scan_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_runs(out_dir,scan,ses=\"\",task=\"\",acq=\"\",ce=\"\",dirs=\"\",rec=\"\",echo=\"\"):\n",
    "    '''\n",
    "    Determines run number of a scan (e.g. T1w, T2w, bold, dwi etc.) in an output directory by globbing the \n",
    "    directory for the number of NifTis of the same scan.\n",
    "\n",
    "    Arguments (required):\n",
    "        out_dir (string): Absolute path to output directory\n",
    "        scan (string): Modality (e.g. T1w, T2w, bold, dwi, etc.)\n",
    "\n",
    "    Arguments (optional):\n",
    "        ses (string): Session ID\n",
    "        task (string): Task ID\n",
    "        acq (string): Acquisition ID\n",
    "        ce (string): Contrast Enhanced ID\n",
    "        dirs (string): Directions ID string\n",
    "        rec (string): Reconstruction algorithm string\n",
    "        echo (int or string): Echo number from multi-echo functional scan\n",
    "\n",
    "    Returns:\n",
    "        run_num (int): Returns the run number for the specific scan\n",
    "    '''\n",
    "\n",
    "    runs = os.path.join(out_dir, f\"*{ses}*{task}*{acq}*{ce}*{dirs}*{rec}*{echo}*{scan}*.nii*\")\n",
    "    run_num = len(glob.glob(runs))\n",
    "    run_num = run_num + 1\n",
    "\n",
    "    return run_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_file(file,work_dir=\"\",work_name=\"\"):\n",
    "    '''\n",
    "    Copies a file. Primarily intended for copying single file image data.\n",
    "    \n",
    "    Arguments:\n",
    "        file (string): File path to source (image) file\n",
    "        work_dir (string): Absolute path to working directory (must exist at runtime prior to invoakation of this function). If left empty, then the directory of the source file is used.\n",
    "        work_name (string): Output name for (image) file. If left empty, the output name is the same as the source file.\n",
    "        \n",
    "    Returns:\n",
    "        out_file (string): Absolute path to output file.\n",
    "    '''\n",
    "    \n",
    "    [path, filename, ext] = file_parts(file)\n",
    "    \n",
    "    if work_dir == \"\":\n",
    "        work_dir = path\n",
    "    else:\n",
    "        work_dir = os.path.abspath(work_dir)\n",
    "        \n",
    "    if work_name == \"\":\n",
    "        work_name = filename\n",
    "        \n",
    "    out_file = os.path.join(work_dir,work_name + ext)\n",
    "    \n",
    "    shutil.copy(file,out_file)\n",
    "    \n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nii_tr(nii_file):\n",
    "    '''\n",
    "    Reads the NifTi file header and returns the repetition time (TR, sec) as a value if it is not zero, otherwise this \n",
    "    function returns the string 'unknown'.\n",
    "    \n",
    "    Arguments:\n",
    "        nii_file (string): NifTi image filename with absolute filepath\n",
    "        \n",
    "    Returns: \n",
    "        tr (float or string): Repetition time (TR, sec), if not zero, otherwise 'unknown' is returned.\n",
    "    '''\n",
    "    \n",
    "    # Load nifti file\n",
    "    img = nib.load(nii_file)\n",
    "    \n",
    "    # Store nifti image TR\n",
    "    tr = float(img.header['pixdim'][4])\n",
    "    \n",
    "    # Check if TR is likely\n",
    "    if tr != 0:\n",
    "        pass\n",
    "    else:\n",
    "        tr = \"unknown\"\n",
    "    \n",
    "    return tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_parts(file):\n",
    "    '''\n",
    "    Divides file with file path into: path, filename, extension.\n",
    "    \n",
    "    Arguments:\n",
    "        file (string): File with absolute filepath\n",
    "        \n",
    "    Returns: \n",
    "        path (string): Path of input file\n",
    "        filename (string): Filename of input file, without the extension\n",
    "        ext (string): Extension of input file\n",
    "    '''\n",
    "    \n",
    "    [path, file_with_ext] = os.path.split(file)\n",
    "    [filename,ext] = os.path.splitext(file_with_ext)\n",
    "    \n",
    "    path = str(path)\n",
    "    filename = str(filename)\n",
    "    ext = str(ext)\n",
    "    \n",
    "    return path,filename,ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gzip_file(file,rm_orig=True):\n",
    "    '''\n",
    "    Gzips file.\n",
    "    \n",
    "    Arguments:\n",
    "        file (string): Input file\n",
    "        rm_orig (boolean): If true (default), removes original file\n",
    "        \n",
    "    Returns: \n",
    "        out_file (string): Gzipped file\n",
    "    '''\n",
    "    \n",
    "    # Define tempory file for I/O buffer stream\n",
    "    tmp_file = file\n",
    "    path,f_name_,ext_ = file_parts(tmp_file)\n",
    "    f_name = f_name_ + ext_ + \".gz\"\n",
    "    out_file = os.path.join(path,f_name)\n",
    "    \n",
    "    # Gzip file\n",
    "    with open(file,\"rb\") as in_file:\n",
    "        data = in_file.read(); in_file.close()\n",
    "        with gzip.GzipFile(out_file,\"wb\") as tmp_out:\n",
    "            tmp_out.write(data)\n",
    "            tmp_out.close()\n",
    "            \n",
    "    if rm_orig:\n",
    "        os.remove(file)\n",
    "            \n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gunzip_file(file,rm_orig=True):\n",
    "    '''\n",
    "    Gunzips file.\n",
    "    \n",
    "    Arguments:\n",
    "        file (string): Input file\n",
    "        rm_orig (boolean): If true (default), removes original file\n",
    "        \n",
    "    Returns: \n",
    "        out_file (string): Gunzipped file\n",
    "    '''\n",
    "    \n",
    "    # Define tempory file for I/O buffer stream\n",
    "    tmp_file = file\n",
    "    path,f_name_,ext_ = file_parts(tmp_file)\n",
    "    f_name = f_name_ # + ext_[:-3]\n",
    "    out_file = os.path.join(path,f_name)\n",
    "    \n",
    "    with gzip.GzipFile(file,\"rb\") as in_file:\n",
    "        data = in_file.read(); in_file.close()\n",
    "        with open(out_file,\"wb\") as tmp_out:\n",
    "            tmp_out.write(data)\n",
    "            tmp_out.close()\n",
    "            \n",
    "    if rm_orig:\n",
    "        os.remove(file)\n",
    "    \n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_json(json_file,dictionary):\n",
    "    '''\n",
    "    Updates JavaScript Object Notation (JSON) file. If the file does not exist, it is created once\n",
    "    this function is invoked.\n",
    "    \n",
    "    Arguments:\n",
    "        json_file (string): Input file\n",
    "        dictionary (dict): Dictionary of key mapped items to write to JSON file\n",
    "        \n",
    "    Returns: \n",
    "        json_file (string): Updated JSON file\n",
    "    '''\n",
    "    \n",
    "    # Check if JSON file exists, if not, then create JSON file\n",
    "    if not os.path.exists(json_file):\n",
    "        with open(json_file,\"w\"): pass\n",
    "        \n",
    "    # Read JSON file\n",
    "    # Try-Except statement has empty exception as JSONDecodeError is not a valid exception to pass, \n",
    "    # thus throwing a name error\n",
    "    try:\n",
    "        with open(json_file) as file:\n",
    "            data_orig = json.load(file)\n",
    "    except:\n",
    "        pass\n",
    "        data_orig = dict()\n",
    "        \n",
    "    # Update original data from JSON file\n",
    "    data_orig.update(dictionary)\n",
    "    \n",
    "    # Write updated JSON file\n",
    "    with open(json_file,\"w\") as file:\n",
    "        json.dump(data_orig,file,indent=4)\n",
    "        \n",
    "    return json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_multi_update(dictionary,**kwargs):\n",
    "    '''\n",
    "    Updates a dictionary multiple times depending on the number key word mapped pairs that are provided and \n",
    "    returns a separate updated dictionary. The dictionary passed as an argument must exist prior to this \n",
    "    function being invoked.\n",
    "    \n",
    "    Example usage:\n",
    "    \n",
    "        new_dict = dict_multi_update(old_dict,\n",
    "                                    Manufacturer=\"Philips\",\n",
    "                                    ManufacturersModelName=\"Ingenia\",\n",
    "                                    MagneticFieldStrength=3,\n",
    "                                    InstitutionName=\"CCHMC\")\n",
    "    \n",
    "    Arguments:\n",
    "        dictionary (dict): Dictionary of key mapped items to write to JSON file\n",
    "        **kwargs (string, key,value pairs): key=value pairs\n",
    "        \n",
    "    Returns: \n",
    "        new_dict (dict): New updated dictionary\n",
    "    '''\n",
    "    \n",
    "    # Create new dictionary\n",
    "    new_dict = dictionary.copy()\n",
    "    \n",
    "    for key,item in kwargs.items():\n",
    "        tmp_dict = {key:item}\n",
    "        new_dict.update(tmp_dict)\n",
    "        \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(dictionary,scan_type=\"\",task=\"\"):\n",
    "    '''\n",
    "    Reads the metadata dictionary and looks for keywords to indicate what metadata should be written to which\n",
    "    dictionary. For example, the keyword 'common' is used to indicate the common information for the imaging\n",
    "    protocol and may contain information such as: field strength, phase encoding direction, institution name, etc.\n",
    "    Additional keywords that are BIDS sub-directories names (e.g. anat, func, dwi) will return an additional\n",
    "    dictionary which contains metadata specific for those modalities. Func also has additional keywords based on\n",
    "    the task specified.\n",
    "    \n",
    "    Arguments:\n",
    "        dictionary (dict): Nest dictionary of key mapped items from the 'read_config' function\n",
    "        scan_type (string): BIDS scan type (e.g. anat, func, dwi, etc., default=\"\")\n",
    "        task (string): Task name to search in the key mapped dictionary\n",
    "        \n",
    "    Returns: \n",
    "        com_param_dict (dict): Common parameters dictionary\n",
    "        scan_param_dict (dict): Scan/modality type parameters dictionary\n",
    "    '''\n",
    "    \n",
    "    # Create empty dictionaries\n",
    "    com_param_dict = dict()\n",
    "    scan_param_dict = dict()\n",
    "    scan_task_dict = dict()\n",
    "    \n",
    "    # Iterate through, looking for key words (e.g. common and scan_type)\n",
    "    for key,item in dictionary.items():\n",
    "        if key.lower() in 'common':\n",
    "            com_param_dict = dictionary[key]\n",
    "\n",
    "        if key.lower() in scan_type:\n",
    "            scan_param_dict = dictionary[key]\n",
    "            if task.lower() in scan_param_dict:\n",
    "                for dict_key,dict_item in scan_param_dict.items():\n",
    "                    if task.lower() in dict_key:\n",
    "                        scan_task_dict = scan_param_dict[dict_key]\n",
    "                        \n",
    "        if len(scan_task_dict) != 0:\n",
    "            scan_param_dict = scan_task_dict\n",
    "    \n",
    "    return com_param_dict, scan_param_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bvals(bval_file):\n",
    "    '''\n",
    "    Reads the bvals from the (FSL-style) bvalue file and returns a list of unique non-zero bvalues\n",
    "    \n",
    "    Arguments:\n",
    "        bval_file (string): Absolute filepath to bval (.bval) file\n",
    "        \n",
    "    Returns: \n",
    "        bvals_list (list): List of unique, non-zero bvalues.\n",
    "    '''\n",
    "    \n",
    "    vals = np.loadtxt(bval_file)\n",
    "    vals_nonzero = vals[vals.astype(bool)]\n",
    "    bvals_list = list(np.unique(vals_nonzero))\n",
    "    \n",
    "    return bvals_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate `EffectiveEchoSpacing` and `TotalReadoutTime`\n",
    "-----\n",
    "\n",
    "See this source for details:https://github.com/bids-standard/bids-specification/blob/master/src/04-modality-specific-files/01-magnetic-resonance-imaging-data.md\n",
    "\n",
    "-----\n",
    "Siemens:        \n",
    "\n",
    "`BWPPPE` = `BandwidthPerPixelPhaseEncode `           \n",
    "\n",
    "`EffectiveEchoSpacing` = 1 / [`BWPPPE` * `ReconMatrixPE`]          \n",
    "\n",
    "`TotalReadoutTime` = `EffectiveEchoSpacing * (ReconMatrixPE - 1)`\n",
    "\n",
    "Philips:\n",
    "\n",
    "`EffectiveEchoSpacing` = (((1000*`WFS`)/(434.215*(`EchoTrainLength`+1)))/`acceleration`)           \n",
    "\n",
    "`TotalReadoutTime` = 0.001 * `EffectiveEchoSpacing` * `EchoTrainLength`\n",
    "\n",
    "See these links for Philips specific details:                 \n",
    "\n",
    "https://www.jiscmail.ac.uk/cgi-bin/webadmin?A2=fsl;162ab1a3.1308           \n",
    "\n",
    "https://support.brainvoyager.com/brainvoyager/functional-analysis-preparation/29-pre-processing/78-epi-distortion-correction-echo-spacing-and-bandwidth           \n",
    "\n",
    "https://neurostars.org/t/consolidating-epi-echo-spacing-and-readout-time-for-philips-scanner/4406            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = \"C:/Users/smart/Desktop/GitProjects/convsauce/IRC287H-8/20171003/1101_rsfMRI_MB6_SENSE_1_fat_shift_P_017100310465322437/MR1101000016.dcm\"\n",
    "# f = \"/Users/brac4g/Desktop/convsauce/IRC287H-8/20171003/1101_rsfMRI_MB6_SENSE_1_fat_shift_P_017100310465322437/MR1101000016.dcm\"\n",
    "f = \"/Users/brac4g/Downloads/MR.1.3.12.2.1107.5.2.19.45307.2017051015422162853047250.dcm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0008, 0005) Specific Character Set              CS: 'ISO_IR 100'\n",
       "(0008, 0008) Image Type                          CS: ['ORIGINAL', 'PRIMARY', 'M', 'ND', 'MOSAIC']\n",
       "(0008, 0012) Instance Creation Date              DA: '20170510'\n",
       "(0008, 0013) Instance Creation Time              TM: '154222.295000'\n",
       "(0008, 0016) SOP Class UID                       UI: MR Image Storage\n",
       "(0008, 0018) SOP Instance UID                    UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162853047250\n",
       "(0008, 0020) Study Date                          DA: '20170510'\n",
       "(0008, 0021) Series Date                         DA: '20170510'\n",
       "(0008, 0022) Acquisition Date                    DA: '20170510'\n",
       "(0008, 0023) Content Date                        DA: '20170510'\n",
       "(0008, 0030) Study Time                          TM: '152243.068000'\n",
       "(0008, 0031) Series Time                         TM: '153725.445000'\n",
       "(0008, 0032) Acquisition Time                    TM: '154158.857500'\n",
       "(0008, 0033) Content Time                        TM: '154222.295000'\n",
       "(0008, 0050) Accession Number                    SH: '84075707'\n",
       "(0008, 0060) Modality                            CS: 'MR'\n",
       "(0008, 0070) Manufacturer                        LO: 'SIEMENS'\n",
       "(0008, 0080) Institution Name                    LO: 'CHP'\n",
       "(0008, 0081) Institution Address                 ST: 'Penn Avenue 4401,Pittsburgh,Pennsylvania,US,15224'\n",
       "(0008, 0090) Referring Physician's Name          PN: 'PANIGRAHY'\n",
       "(0008, 1010) Station Name                        SH: 'CHPMR5'\n",
       "(0008, 1030) Study Description                   LO: 'MR BRAIN WITHOUT CONTRAST'\n",
       "(0008, 1032)  Procedure Code Sequence   1 item(s) ---- \n",
       "   (0008, 0100) Code Value                          SH: 'MRBRAINX'\n",
       "   (0008, 0102) Coding Scheme Designator            SH: 'LOCAL'\n",
       "   (0008, 0104) Code Meaning                        LO: 'MR BRAIN WITHOUT CONTRAST'\n",
       "   ---------\n",
       "(0008, 103e) Series Description                  LO: 'MULTIBAND_BOLD #1'\n",
       "(0008, 1040) Institutional Department Name       LO: 'Department'\n",
       "(0008, 1048) Physician(s) of Record              PN: 'PANIGRAHY ASHOK'\n",
       "(0008, 1050) Performing Physician's Name         PN: 'PANIGRAHY ASHOK'\n",
       "(0008, 1070) Operators' Name                     PN: 'JBB  SIEMENS'\n",
       "(0008, 1090) Manufacturer's Model Name           LO: 'Skyra'\n",
       "(0008, 1110)  Referenced Study Sequence   1 item(s) ---- \n",
       "   (0008, 1150) Referenced SOP Class UID            UI: Detached Study Management SOP Class\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 2.16.840.1.114151.2.2.10.42842.6429.6720287\n",
       "   ---------\n",
       "(0008, 1120)  Referenced Patient Sequence   1 item(s) ---- \n",
       "   (0008, 1150) Referenced SOP Class UID            UI: Detached Patient Management SOP Class\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 2.16.840.1.114151.2.2.10.42842.6429.6720286\n",
       "   ---------\n",
       "(0008, 1140)  Referenced Image Sequence   3 item(s) ---- \n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015230255727871234\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015230767730971242\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015230511779371238\n",
       "   ---------\n",
       "(0008, 2112)  Source Image Sequence   9 item(s) ---- \n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162465247230\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162489147232\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162508347234\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162530447237\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162569647240\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162696847246\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162701147247\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162708247248\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162711747249\n",
       "   ---------\n",
       "(0010, 0010) Patient's Name                      PN: 'LEPAGE^KALEB^'\n",
       "(0010, 0020) Patient ID                          LO: '841132539'\n",
       "(0010, 0030) Patient's Birth Date                DA: '19990516'\n",
       "(0010, 0040) Patient's Sex                       CS: 'M'\n",
       "(0010, 1010) Patient's Age                       AS: '017Y'\n",
       "(0010, 1020) Patient's Size                      DS: \"1.813\"\n",
       "(0010, 1030) Patient's Weight                    DS: \"61.2\"\n",
       "(0018, 0015) Body Part Examined                  CS: 'HEAD'\n",
       "(0018, 0020) Scanning Sequence                   CS: 'EP'\n",
       "(0018, 0021) Sequence Variant                    CS: 'SK'\n",
       "(0018, 0022) Scan Options                        CS: ['PFP', 'FS']\n",
       "(0018, 0023) MR Acquisition Type                 CS: '2D'\n",
       "(0018, 0024) Sequence Name                       SH: 'epfid2d1_64'\n",
       "(0018, 0025) Angio Flag                          CS: 'N'\n",
       "(0018, 0050) Slice Thickness                     DS: \"4\"\n",
       "(0018, 0080) Repetition Time                     DS: \"650\"\n",
       "(0018, 0081) Echo Time                           DS: \"32\"\n",
       "(0018, 0083) Number of Averages                  DS: \"1\"\n",
       "(0018, 0084) Imaging Frequency                   DS: \"123.247555\"\n",
       "(0018, 0085) Imaged Nucleus                      SH: '1H'\n",
       "(0018, 0086) Echo Number(s)                      IS: \"1\"\n",
       "(0018, 0087) Magnetic Field Strength             DS: \"3\"\n",
       "(0018, 0088) Spacing Between Slices              DS: \"3.9999997615814\"\n",
       "(0018, 0089) Number of Phase Encoding Steps      IS: \"48\"\n",
       "(0018, 0091) Echo Train Length                   IS: \"48\"\n",
       "(0018, 0093) Percent Sampling                    DS: \"100\"\n",
       "(0018, 0094) Percent Phase Field of View         DS: \"100\"\n",
       "(0018, 0095) Pixel Bandwidth                     DS: \"2440\"\n",
       "(0018, 1000) Device Serial Number                LO: '45307'\n",
       "(0018, 1020) Software Version(s)                 LO: 'syngo MR E11'\n",
       "(0018, 1030) Protocol Name                       LO: 'MULTIBAND_BOLD #1'\n",
       "(0018, 1251) Transmit Coil Name                  SH: 'Body'\n",
       "(0018, 1310) Acquisition Matrix                  US: [64, 0, 0, 64]\n",
       "(0018, 1312) In-plane Phase Encoding Direction   CS: 'COL'\n",
       "(0018, 1314) Flip Angle                          DS: \"50\"\n",
       "(0018, 1315) Variable Flip Angle Flag            CS: 'N'\n",
       "(0018, 1316) SAR                                 DS: \"0.06818476817093\"\n",
       "(0018, 1318) dB/dt                               DS: \"0\"\n",
       "(0018, 5100) Patient Position                    CS: 'HFS'\n",
       "(0019, 0010) Private Creator                     LO: 'SIEMENS MR HEADER'\n",
       "(0019, 1008) [CSA Image Header Type]             CS: 'IMAGE NUM 4'\n",
       "(0019, 1009) [CSA Image Header Version ??]       LO: '1.0'\n",
       "(0019, 100a) [NumberOfImagesInMosaic]            US: 9\n",
       "(0019, 100b) [SliceMeasurementDuration]          DS: \"309000\"\n",
       "(0019, 100f) [GradientMode]                      SH: 'Fast'\n",
       "(0019, 1011) [FlowCompensation]                  SH: 'No'\n",
       "(0019, 1012) [TablePositionOrigin]               SL: [0, 0, -1082]\n",
       "(0019, 1013) [ImaAbsTablePosition]               SL: [0, 0, -1082]\n",
       "(0019, 1014) [ImaRelTablePosition]               IS: ['0', '0', '-108']\n",
       "(0019, 1015) [SlicePosition_PCS]                 FD: [-384.0, -383.32203388, 50.64406967]\n",
       "(0019, 1016) [TimeAfterStart]                    DS: \"305.1025\"\n",
       "(0019, 1017) [SliceResolution]                   DS: \"1\"\n",
       "(0019, 1018) [RealDwellTime]                     IS: \"3200\"\n",
       "(0019, 1028) [BandwidthPerPixelPhaseEncode]      FD: 29.481\n",
       "(0019, 1029) [MosaicRefAcqTimes]                 FD: [0.0, 349.99999998, 69.99999998, 419.99999998, 139.99999998, 489.99999999, 209.99999999, 560.0, 280.0]\n",
       "(0020, 000d) Study Instance UID                  UI: 2.16.840.1.114151.2.2.10.42842.6429.6720287\n",
       "(0020, 000e) Series Instance UID                 UI: 1.3.12.2.1107.5.2.19.45307.201705101536516573384267.0.0.0\n",
       "(0020, 0010) Study ID                            SH: '84075707'\n",
       "(0020, 0011) Series Number                       IS: \"5\"\n",
       "(0020, 0012) Acquisition Number                  IS: \"470\"\n",
       "(0020, 0013) Instance Number                     IS: \"470\"\n",
       "(0020, 0032) Image Position (Patient)            DS: ['-384', '-383.32203388214', '-57.355930328369']\n",
       "(0020, 0037) Image Orientation (Patient)         DS: ['1', '0', '0', '0', '1', '0']\n",
       "(0020, 0052) Frame of Reference UID              UI: 1.3.12.2.1107.5.2.19.45307.1.20170510152243279.0.0.0\n",
       "(0020, 1040) Position Reference Indicator        LO: ''\n",
       "(0020, 1041) Slice Location                      DS: \"-57.355930328369\"\n",
       "(0028, 0002) Samples per Pixel                   US: 1\n",
       "(0028, 0004) Photometric Interpretation          CS: 'MONOCHROME2'\n",
       "(0028, 0010) Rows                                US: 192\n",
       "(0028, 0011) Columns                             US: 192\n",
       "(0028, 0030) Pixel Spacing                       DS: ['4', '4']\n",
       "(0028, 0100) Bits Allocated                      US: 16\n",
       "(0028, 0101) Bits Stored                         US: 12\n",
       "(0028, 0102) High Bit                            US: 11\n",
       "(0028, 0103) Pixel Representation                US: 0\n",
       "(0028, 0106) Smallest Image Pixel Value          US: 0\n",
       "(0028, 0107) Largest Image Pixel Value           US: 2002\n",
       "(0028, 1050) Window Center                       DS: \"645\"\n",
       "(0028, 1051) Window Width                        DS: \"1467\"\n",
       "(0028, 1055) Window Center & Width Explanation   LO: 'Algo1'\n",
       "(0029, 0010) Private Creator                     LO: 'SIEMENS CSA HEADER'\n",
       "(0029, 0011) Private Creator                     LO: 'SIEMENS MEDCOM HEADER2'\n",
       "(0029, 1008) [CSA Image Header Type]             CS: 'IMAGE NUM 4'\n",
       "(0029, 1009) [CSA Image Header Version]          LO: '20170510'\n",
       "(0029, 1010) [CSA Image Header Info]             OB: Array of 11704 elements\n",
       "(0029, 1018) [CSA Series Header Type]            CS: 'MR'\n",
       "(0029, 1019) [CSA Series Header Version]         LO: '20170510'\n",
       "(0029, 1020) [CSA Series Header Info]            OB: Array of 114112 elements\n",
       "(0029, 1160) [Series Workflow Status]            LO: 'com'\n",
       "(0032, 1032) Requesting Physician                PN: 'PANIGRAHY'\n",
       "(0032, 1060) Requested Procedure Description     LO: 'MR BRAIN WITHOUT CONTRAST'\n",
       "(0032, 1064)  Requested Procedure Code Sequence   1 item(s) ---- \n",
       "   (0008, 0100) Code Value                          SH: 'MRBRAINX'\n",
       "   (0008, 0102) Coding Scheme Designator            SH: 'LOCAL'\n",
       "   (0008, 0104) Code Meaning                        LO: 'MR BRAIN WITHOUT CONTRAST'\n",
       "   ---------\n",
       "(0040, 0244) Performed Procedure Step Start Date DA: '20170510'\n",
       "(0040, 0245) Performed Procedure Step Start Time TM: '152243.193000'\n",
       "(0040, 0253) Performed Procedure Step ID         SH: '2723193'\n",
       "(0040, 0254) Performed Procedure Step Descriptio LO: 'MR BRAIN WITHOUT CONTRAST'\n",
       "(0040, 0275)  Request Attributes Sequence   1 item(s) ---- \n",
       "   (0040, 0007) Scheduled Procedure Step Descriptio LO: 'MR BRAIN WITHOUT CONTRAST'\n",
       "   (0040, 0008)  Scheduled Protocol Code Sequence   1 item(s) ---- \n",
       "\n",
       "      ---------\n",
       "   (0040, 0009) Scheduled Procedure Step ID         SH: '2723193'\n",
       "   (0040, 1001) Requested Procedure ID              SH: '84075707'\n",
       "   ---------\n",
       "(0040, 0280) Comments on the Performed Procedure ST: ''\n",
       "(0051, 0010) Private Creator                     LO: 'SIEMENS MR HEADER'\n",
       "(0051, 1008) [CSA Image Header Type]             CS: 'IMAGE NUM 4'\n",
       "(0051, 1009) [CSA Image Header Version ??]       LO: '1.0'\n",
       "(0051, 100a) [Unknown]                           LO: 'TA 05:09'\n",
       "(0051, 100b) [AcquisitionMatrixText]             LO: '64p*64'\n",
       "(0051, 100c) [Unknown]                           LO: 'FoV 768*768'\n",
       "(0051, 100e) [Unknown]                           LO: 'Tra'\n",
       "(0051, 100f) [CoilString]                        LO: 'HEA;HEP'\n",
       "(0051, 1012) [Unknown]                           SH: 'TP F108'\n",
       "(0051, 1013) [PositivePCSDirections]             SH: '+LPH'\n",
       "(0051, 1016) [Unknown]                           LO: 'M/ND/MOSAIC'\n",
       "(0051, 1017) [Unknown]                           SH: 'SL 4.0'\n",
       "(0051, 1019) [Unknown]                           LO: 'A1/PFP/FS'\n",
       "(0073, 0010) Private Creator                     LO: 'STENTOR'\n",
       "(0073, 1001) [Unknown]                           ST: 'unknown'\n",
       "(0073, 1002) [Private Creator]                   ST: '10.40.16.5'\n",
       "(0073, 1003) [Stentor Remote AETitle Element]    ST: 'MR4_CHP'\n",
       "(0073, 1006) [Stentor Transfer Syntax Value]     LO: '1.2.840.10008.1.2.1'\n",
       "(7fe0, 0010) Pixel Data                          OW: Array of 73728 elements"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pydicom.dcmread(f)\n",
    "# ds = pydicom.dcmread(dcm_file_list_currated[0])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0019, 1028) [BandwidthPerPixelPhaseEncode]      FD: 29.481"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0x0019, 0x1028]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(0019, 1028) [BandwidthPerPixelPhaseEncode]      FD: 29.481'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = str(ds[0x0019, 0x1028])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Dataset.dir of (0008, 0005) Specific Character Set              CS: 'ISO_IR 100'\n",
       "(0008, 0008) Image Type                          CS: ['ORIGINAL', 'PRIMARY', 'M', 'ND', 'MOSAIC']\n",
       "(0008, 0012) Instance Creation Date              DA: '20170510'\n",
       "(0008, 0013) Instance Creation Time              TM: '154222.295000'\n",
       "(0008, 0016) SOP Class UID                       UI: MR Image Storage\n",
       "(0008, 0018) SOP Instance UID                    UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162853047250\n",
       "(0008, 0020) Study Date                          DA: '20170510'\n",
       "(0008, 0021) Series Date                         DA: '20170510'\n",
       "(0008, 0022) Acquisition Date                    DA: '20170510'\n",
       "(0008, 0023) Content Date                        DA: '20170510'\n",
       "(0008, 0030) Study Time                          TM: '152243.068000'\n",
       "(0008, 0031) Series Time                         TM: '153725.445000'\n",
       "(0008, 0032) Acquisition Time                    TM: '154158.857500'\n",
       "(0008, 0033) Content Time                        TM: '154222.295000'\n",
       "(0008, 0050) Accession Number                    SH: '84075707'\n",
       "(0008, 0060) Modality                            CS: 'MR'\n",
       "(0008, 0070) Manufacturer                        LO: 'SIEMENS'\n",
       "(0008, 0080) Institution Name                    LO: 'CHP'\n",
       "(0008, 0081) Institution Address                 ST: 'Penn Avenue 4401,Pittsburgh,Pennsylvania,US,15224'\n",
       "(0008, 0090) Referring Physician's Name          PN: 'PANIGRAHY'\n",
       "(0008, 1010) Station Name                        SH: 'CHPMR5'\n",
       "(0008, 1030) Study Description                   LO: 'MR BRAIN WITHOUT CONTRAST'\n",
       "(0008, 1032)  Procedure Code Sequence   1 item(s) ---- \n",
       "   (0008, 0100) Code Value                          SH: 'MRBRAINX'\n",
       "   (0008, 0102) Coding Scheme Designator            SH: 'LOCAL'\n",
       "   (0008, 0104) Code Meaning                        LO: 'MR BRAIN WITHOUT CONTRAST'\n",
       "   ---------\n",
       "(0008, 103e) Series Description                  LO: 'MULTIBAND_BOLD #1'\n",
       "(0008, 1040) Institutional Department Name       LO: 'Department'\n",
       "(0008, 1048) Physician(s) of Record              PN: 'PANIGRAHY ASHOK'\n",
       "(0008, 1050) Performing Physician's Name         PN: 'PANIGRAHY ASHOK'\n",
       "(0008, 1070) Operators' Name                     PN: 'JBB  SIEMENS'\n",
       "(0008, 1090) Manufacturer's Model Name           LO: 'Skyra'\n",
       "(0008, 1110)  Referenced Study Sequence   1 item(s) ---- \n",
       "   (0008, 1150) Referenced SOP Class UID            UI: Detached Study Management SOP Class\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 2.16.840.1.114151.2.2.10.42842.6429.6720287\n",
       "   ---------\n",
       "(0008, 1120)  Referenced Patient Sequence   1 item(s) ---- \n",
       "   (0008, 1150) Referenced SOP Class UID            UI: Detached Patient Management SOP Class\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 2.16.840.1.114151.2.2.10.42842.6429.6720286\n",
       "   ---------\n",
       "(0008, 1140)  Referenced Image Sequence   3 item(s) ---- \n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015230255727871234\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015230767730971242\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015230511779371238\n",
       "   ---------\n",
       "(0008, 2112)  Source Image Sequence   9 item(s) ---- \n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162465247230\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162489147232\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162508347234\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162530447237\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162569647240\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162696847246\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162701147247\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162708247248\n",
       "   ---------\n",
       "   (0008, 1150) Referenced SOP Class UID            UI: MR Image Storage\n",
       "   (0008, 1155) Referenced SOP Instance UID         UI: 1.3.12.2.1107.5.2.19.45307.2017051015422162711747249\n",
       "   ---------\n",
       "(0010, 0010) Patient's Name                      PN: 'LEPAGE^KALEB^'\n",
       "(0010, 0020) Patient ID                          LO: '841132539'\n",
       "(0010, 0030) Patient's Birth Date                DA: '19990516'\n",
       "(0010, 0040) Patient's Sex                       CS: 'M'\n",
       "(0010, 1010) Patient's Age                       AS: '017Y'\n",
       "(0010, 1020) Patient's Size                      DS: \"1.813\"\n",
       "(0010, 1030) Patient's Weight                    DS: \"61.2\"\n",
       "(0018, 0015) Body Part Examined                  CS: 'HEAD'\n",
       "(0018, 0020) Scanning Sequence                   CS: 'EP'\n",
       "(0018, 0021) Sequence Variant                    CS: 'SK'\n",
       "(0018, 0022) Scan Options                        CS: ['PFP', 'FS']\n",
       "(0018, 0023) MR Acquisition Type                 CS: '2D'\n",
       "(0018, 0024) Sequence Name                       SH: 'epfid2d1_64'\n",
       "(0018, 0025) Angio Flag                          CS: 'N'\n",
       "(0018, 0050) Slice Thickness                     DS: \"4\"\n",
       "(0018, 0080) Repetition Time                     DS: \"650\"\n",
       "(0018, 0081) Echo Time                           DS: \"32\"\n",
       "(0018, 0083) Number of Averages                  DS: \"1\"\n",
       "(0018, 0084) Imaging Frequency                   DS: \"123.247555\"\n",
       "(0018, 0085) Imaged Nucleus                      SH: '1H'\n",
       "(0018, 0086) Echo Number(s)                      IS: \"1\"\n",
       "(0018, 0087) Magnetic Field Strength             DS: \"3\"\n",
       "(0018, 0088) Spacing Between Slices              DS: \"3.9999997615814\"\n",
       "(0018, 0089) Number of Phase Encoding Steps      IS: \"48\"\n",
       "(0018, 0091) Echo Train Length                   IS: \"48\"\n",
       "(0018, 0093) Percent Sampling                    DS: \"100\"\n",
       "(0018, 0094) Percent Phase Field of View         DS: \"100\"\n",
       "(0018, 0095) Pixel Bandwidth                     DS: \"2440\"\n",
       "(0018, 1000) Device Serial Number                LO: '45307'\n",
       "(0018, 1020) Software Version(s)                 LO: 'syngo MR E11'\n",
       "(0018, 1030) Protocol Name                       LO: 'MULTIBAND_BOLD #1'\n",
       "(0018, 1251) Transmit Coil Name                  SH: 'Body'\n",
       "(0018, 1310) Acquisition Matrix                  US: [64, 0, 0, 64]\n",
       "(0018, 1312) In-plane Phase Encoding Direction   CS: 'COL'\n",
       "(0018, 1314) Flip Angle                          DS: \"50\"\n",
       "(0018, 1315) Variable Flip Angle Flag            CS: 'N'\n",
       "(0018, 1316) SAR                                 DS: \"0.06818476817093\"\n",
       "(0018, 1318) dB/dt                               DS: \"0\"\n",
       "(0018, 5100) Patient Position                    CS: 'HFS'\n",
       "(0019, 0010) Private Creator                     LO: 'SIEMENS MR HEADER'\n",
       "(0019, 1008) [CSA Image Header Type]             CS: 'IMAGE NUM 4'\n",
       "(0019, 1009) [CSA Image Header Version ??]       LO: '1.0'\n",
       "(0019, 100a) [NumberOfImagesInMosaic]            US: 9\n",
       "(0019, 100b) [SliceMeasurementDuration]          DS: \"309000\"\n",
       "(0019, 100f) [GradientMode]                      SH: 'Fast'\n",
       "(0019, 1011) [FlowCompensation]                  SH: 'No'\n",
       "(0019, 1012) [TablePositionOrigin]               SL: [0, 0, -1082]\n",
       "(0019, 1013) [ImaAbsTablePosition]               SL: [0, 0, -1082]\n",
       "(0019, 1014) [ImaRelTablePosition]               IS: ['0', '0', '-108']\n",
       "(0019, 1015) [SlicePosition_PCS]                 FD: [-384.0, -383.32203388, 50.64406967]\n",
       "(0019, 1016) [TimeAfterStart]                    DS: \"305.1025\"\n",
       "(0019, 1017) [SliceResolution]                   DS: \"1\"\n",
       "(0019, 1018) [RealDwellTime]                     IS: \"3200\"\n",
       "(0019, 1028) [BandwidthPerPixelPhaseEncode]      FD: 29.481\n",
       "(0019, 1029) [MosaicRefAcqTimes]                 FD: [0.0, 349.99999998, 69.99999998, 419.99999998, 139.99999998, 489.99999999, 209.99999999, 560.0, 280.0]\n",
       "(0020, 000d) Study Instance UID                  UI: 2.16.840.1.114151.2.2.10.42842.6429.6720287\n",
       "(0020, 000e) Series Instance UID                 UI: 1.3.12.2.1107.5.2.19.45307.201705101536516573384267.0.0.0\n",
       "(0020, 0010) Study ID                            SH: '84075707'\n",
       "(0020, 0011) Series Number                       IS: \"5\"\n",
       "(0020, 0012) Acquisition Number                  IS: \"470\"\n",
       "(0020, 0013) Instance Number                     IS: \"470\"\n",
       "(0020, 0032) Image Position (Patient)            DS: ['-384', '-383.32203388214', '-57.355930328369']\n",
       "(0020, 0037) Image Orientation (Patient)         DS: ['1', '0', '0', '0', '1', '0']\n",
       "(0020, 0052) Frame of Reference UID              UI: 1.3.12.2.1107.5.2.19.45307.1.20170510152243279.0.0.0\n",
       "(0020, 1040) Position Reference Indicator        LO: ''\n",
       "(0020, 1041) Slice Location                      DS: \"-57.355930328369\"\n",
       "(0028, 0002) Samples per Pixel                   US: 1\n",
       "(0028, 0004) Photometric Interpretation          CS: 'MONOCHROME2'\n",
       "(0028, 0010) Rows                                US: 192\n",
       "(0028, 0011) Columns                             US: 192\n",
       "(0028, 0030) Pixel Spacing                       DS: ['4', '4']\n",
       "(0028, 0100) Bits Allocated                      US: 16\n",
       "(0028, 0101) Bits Stored                         US: 12\n",
       "(0028, 0102) High Bit                            US: 11\n",
       "(0028, 0103) Pixel Representation                US: 0\n",
       "(0028, 0106) Smallest Image Pixel Value          US: 0\n",
       "(0028, 0107) Largest Image Pixel Value           US: 2002\n",
       "(0028, 1050) Window Center                       DS: \"645\"\n",
       "(0028, 1051) Window Width                        DS: \"1467\"\n",
       "(0028, 1055) Window Center & Width Explanation   LO: 'Algo1'\n",
       "(0029, 0010) Private Creator                     LO: 'SIEMENS CSA HEADER'\n",
       "(0029, 0011) Private Creator                     LO: 'SIEMENS MEDCOM HEADER2'\n",
       "(0029, 1008) [CSA Image Header Type]             CS: 'IMAGE NUM 4'\n",
       "(0029, 1009) [CSA Image Header Version]          LO: '20170510'\n",
       "(0029, 1010) [CSA Image Header Info]             OB: Array of 11704 elements\n",
       "(0029, 1018) [CSA Series Header Type]            CS: 'MR'\n",
       "(0029, 1019) [CSA Series Header Version]         LO: '20170510'\n",
       "(0029, 1020) [CSA Series Header Info]            OB: Array of 114112 elements\n",
       "(0029, 1160) [Series Workflow Status]            LO: 'com'\n",
       "(0032, 1032) Requesting Physician                PN: 'PANIGRAHY'\n",
       "(0032, 1060) Requested Procedure Description     LO: 'MR BRAIN WITHOUT CONTRAST'\n",
       "(0032, 1064)  Requested Procedure Code Sequence   1 item(s) ---- \n",
       "   (0008, 0100) Code Value                          SH: 'MRBRAINX'\n",
       "   (0008, 0102) Coding Scheme Designator            SH: 'LOCAL'\n",
       "   (0008, 0104) Code Meaning                        LO: 'MR BRAIN WITHOUT CONTRAST'\n",
       "   ---------\n",
       "(0040, 0244) Performed Procedure Step Start Date DA: '20170510'\n",
       "(0040, 0245) Performed Procedure Step Start Time TM: '152243.193000'\n",
       "(0040, 0253) Performed Procedure Step ID         SH: '2723193'\n",
       "(0040, 0254) Performed Procedure Step Descriptio LO: 'MR BRAIN WITHOUT CONTRAST'\n",
       "(0040, 0275)  Request Attributes Sequence   1 item(s) ---- \n",
       "   (0040, 0007) Scheduled Procedure Step Descriptio LO: 'MR BRAIN WITHOUT CONTRAST'\n",
       "   (0040, 0008)  Scheduled Protocol Code Sequence   1 item(s) ---- \n",
       "\n",
       "      ---------\n",
       "   (0040, 0009) Scheduled Procedure Step ID         SH: '2723193'\n",
       "   (0040, 1001) Requested Procedure ID              SH: '84075707'\n",
       "   ---------\n",
       "(0040, 0280) Comments on the Performed Procedure ST: ''\n",
       "(0051, 0010) Private Creator                     LO: 'SIEMENS MR HEADER'\n",
       "(0051, 1008) [CSA Image Header Type]             CS: 'IMAGE NUM 4'\n",
       "(0051, 1009) [CSA Image Header Version ??]       LO: '1.0'\n",
       "(0051, 100a) [Unknown]                           LO: 'TA 05:09'\n",
       "(0051, 100b) [AcquisitionMatrixText]             LO: '64p*64'\n",
       "(0051, 100c) [Unknown]                           LO: 'FoV 768*768'\n",
       "(0051, 100e) [Unknown]                           LO: 'Tra'\n",
       "(0051, 100f) [CoilString]                        LO: 'HEA;HEP'\n",
       "(0051, 1012) [Unknown]                           SH: 'TP F108'\n",
       "(0051, 1013) [PositivePCSDirections]             SH: '+LPH'\n",
       "(0051, 1016) [Unknown]                           LO: 'M/ND/MOSAIC'\n",
       "(0051, 1017) [Unknown]                           SH: 'SL 4.0'\n",
       "(0051, 1019) [Unknown]                           LO: 'A1/PFP/FS'\n",
       "(0073, 0010) Private Creator                     LO: 'STENTOR'\n",
       "(0073, 1001) [Unknown]                           ST: 'unknown'\n",
       "(0073, 1002) [Private Creator]                   ST: '10.40.16.5'\n",
       "(0073, 1003) [Stentor Remote AETitle Element]    ST: 'MR4_CHP'\n",
       "(0073, 1006) [Stentor Transfer Syntax Value]     LO: '1.2.840.10008.1.2.1'\n",
       "(7fe0, 0010) Pixel Data                          OW: Array of 73728 elements>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bwpppe(dcm_file):\n",
    "    '''\n",
    "    Reads the Bandwidth Per Pixel PhaseEncode value from a DICOM header. \n",
    "    \n",
    "    Note: This DICOM field is usually left blank on Philips DICOM headers.\n",
    "    \n",
    "    Arguments:\n",
    "        dcm_file (string): Absolute filepath to DICOM file\n",
    "        \n",
    "    Returns:\n",
    "        bwpppe (float or string): Bandwidth Per Pixel PhaseEncode value\n",
    "    '''\n",
    "    \n",
    "    # Load data\n",
    "    ds = pydicom.dcmread(dcm_file)\n",
    "    \n",
    "    # Get relevant DICOM field\n",
    "    try:\n",
    "        val_str = str(ds[0x0019, 0x1028])\n",
    "        val_list = val_str.split(\" \")\n",
    "        bwpppe = val_list[-1]\n",
    "        bwpppe = float(bwpppe)\n",
    "    except (AttributeError,KeyError):\n",
    "        bwpppe = 'unknown'\n",
    "        pass\n",
    "    \n",
    "    return bwpppe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recon_mat(json_file):\n",
    "    '''\n",
    "    Reads ReconMatrixPE (reconstruction matrix phase encode) value from the JSON sidecar.\n",
    "    \n",
    "    Arguments:\n",
    "        json_file (string): Absolute filepath to JSON file\n",
    "        \n",
    "    Returns:\n",
    "        recon_mat (float or string): Recon Matrix PE value\n",
    "    '''\n",
    "    \n",
    "    # Read JSON file\n",
    "    # Try-Except statement has empty exception as JSONDecodeError is not a valid exception to pass, \n",
    "    # thus throwing a name error\n",
    "    try:\n",
    "        with open(json_file, \"r\") as read_file:\n",
    "            data = json.load(read_file)\n",
    "            recon_mat = data[\"ReconMatrixPE\"]\n",
    "    except:\n",
    "        recon_mat = 'unknown'\n",
    "        pass\n",
    "    \n",
    "    return recon_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pix_band(json_file):\n",
    "    '''\n",
    "    Reads pixel bandwidth value from the JSON sidecar.\n",
    "    \n",
    "    Arguments:\n",
    "        json_file (string): Absolute filepath to JSON file\n",
    "        \n",
    "    Returns:\n",
    "        pix_band (float or string): Pixel bandwidth value\n",
    "    '''\n",
    "    \n",
    "    # Read JSON file\n",
    "    # Try-Except statement has empty exception as JSONDecodeError is not a valid exception to pass, \n",
    "    # thus throwing a name error\n",
    "    try:\n",
    "        with open(json_file, \"r\") as read_file:\n",
    "            data = json.load(read_file)\n",
    "            pix_band = data[\"PixelBandwidth\"]\n",
    "    except:\n",
    "        pix_band = 'unknown'\n",
    "        pass\n",
    "    \n",
    "    return pix_band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Theoretical) Echo Space:\n",
    "# 1/BW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004098360655737705"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Siemens\n",
    "1/2440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000649772579597141"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Philips\n",
    "1/1539"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005300023744106374"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Effective Echo Spacing Siemens\n",
    "1/(29.481*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.033390149587870156"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total Readout time Siemens\n",
    "(1/(29.481*64))*(64-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00040343237704918035"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Siemens effective echo spacing with approaches 3 and 4\n",
    "((1/(2440*64))*(64-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025416239754098364"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Siemens total readout time with approaches 3 and 4\n",
    "(0.00040343237704918035)*(64-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.37328696502401"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percent error effective echo spacing\n",
    "((abs(0.0005300023744106374 - 0.00040343237704918035))/0.00040343237704918035) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.37328696502401"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percent error total readout time\n",
    "((abs(0.033390149587870156 - 0.025416239754098364)/0.025416239754098364)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There approx. 30% error. Maybe try using a fudge factor of some sort, perhaps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005244620901639344"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Siemens effective echo spacing with approaches 3 and 4 (with 30% fudge factor)\n",
    "((1/(2440*64))*(64-1)) * 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03304111168032787"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Siemens total readout time with approaches 3 and 4\n",
    "(0.0005244620901639344)*(64-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3732869650240196"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percent error effective echo spacing\n",
    "((abs(0.0005300023744106374 - 0.0005244620901639344))/0.00040343237704918035) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3732869650240276"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percent error total readout time\n",
    "((abs(0.033390149587870156 - 0.03304111168032787)/0.025416239754098364)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.15172647319917e-06"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Effective Echo Spacing Philips dicom (not reasonable)\n",
    "1/(1539*71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006406208531239419"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Effective Echo spacing (reasonable, approaches 3 and 4)\n",
    "((1/(1539*71))*(71-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04484345971867593"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total Readout time philips dicom (reasonable, approaches 3 and 4...acceptable?)\n",
    "(0.0006406208531239419)*(71-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_read_time(file, json_file=\"\"):\n",
    "    '''\n",
    "    Calculates the effective echo spacing and total readout time provided several combinations of parameters.\n",
    "    Several approaches and methods to calculating the effective echo spacing and total readout within this function\n",
    "    differ and are dependent on the parameters found within the provided JSON sidecar. Currently, there a four \n",
    "    approaches for calculating the effective echo space (all with differing values) and two ways of calculating \n",
    "    the total readout time. It should also be noted that several of these approaches are vendor specific (e.g. at \n",
    "    the time of writing, 16 Jan 2019, the necessary information for approach 1 is only found in Siemens DICOM \n",
    "    headers - the necessary information for approach 2 is only possible if the data is stored in PAR REC format as the\n",
    "    WaterFatShift is a private tag in the Philips DICOM header - approaches 3 and 4 are intended for Philips/GE DICOMs \n",
    "    as those values are anticipated to exist in their DICOM headers).\n",
    "    \n",
    "    The approaches are listed below:\n",
    "    \n",
    "    Approach 1 (BIDS method, Siemens):\n",
    "        BWPPPE = BandwidthPerPixelPhaseEncode\n",
    "        EffectiveEchoSpacing = 1/[BWPPPE * ReconMatrixPE]\n",
    "        TotalReadoutTime = EffectiveEchoSpacing * (ReconMatrixPE - 1)\n",
    "        \n",
    "    Approach 2 (Philips method - PAR REC):\n",
    "        EffectiveEchoSpacing = (((1000 * WaterFatShift)/(434.215 * (EchoTrainLength + 1)))/ParallelReductionFactorInPlane)\n",
    "        TotalReadoutTime = 0.001 * EffectiveEchoSpacing * EchoTrainLength\n",
    "    \n",
    "    Approach 3 (Philips/GE method - DICOM):\n",
    "        EffectiveEchoSpacing = ((1/(PixelBandwidth * EchoTrainLength)) * (EchoTrainLength - 1)) * 1.3\n",
    "        TotalReadoutTime = EffectiveEchoSpacing * (EchoTrainLength - 1)\n",
    "        \n",
    "    Approach 4 (Philips/GE method - DICOM):\n",
    "        EffectiveEchoSpacing = ((1/(PixelBandwidth * ReconMatrixPE)) * (ReconMatrixPE - 1)) * 1.3\n",
    "        tot_read_time = EffectiveEchoSpacing * (ReconMatrixPE - 1)\n",
    "        \n",
    "        Note: EchoTrainLength is assumed to be equal to ReconMatrixPE for approaches 3 and 4, as these values are generally close.\n",
    "        Note: Approaches 3 and 4 appear to have about a 30% decrease in Siemens data when this was tested. The solution was to implement a fudge factor that accounted for the 30% decrease.\n",
    "    \n",
    "    Arguments:\n",
    "        file (string): Absolute filepath to raw image data file (DICOM or PAR REC)\n",
    "        json_file (string, optional): Absolute filepath to JSON sidecare\n",
    "        \n",
    "    Returns:\n",
    "        eff_echo_sp (float): Effective Echo Spacing\n",
    "        tot_read_time (float): Total Readout Time\n",
    "        \n",
    "    References:\n",
    "    Approach 1: https://github.com/bids-standard/bids-specification/blob/master/src/04-modality-specific-files/01-magnetic-resonance-imaging-data.md\n",
    "    Approach 2: https://osf.io/hks7x/ - page 7; \n",
    "    https://support.brainvoyager.com/brainvoyager/functional-analysis-preparation/29-pre-processing/78-epi-distortion-correction-echo-spacing-and-bandwidth\n",
    "    \n",
    "    Forum that raised this specific issue with Philips: https://neurostars.org/t/consolidating-epi-echo-spacing-and-readout-time-for-philips-scanner/4406\n",
    "    \n",
    "    Approaches 3 and 4 were found thorugh trial and error and yielded similar, but not the same values as approaches 1 and 2.\n",
    "    '''\n",
    "    \n",
    "    # check file extension\n",
    "    if 'dcm' in file:\n",
    "        calc_method = 'dcm'\n",
    "    elif 'PAR' in file:\n",
    "        calc_method = 'par'\n",
    "        \n",
    "    # Create empty string variables\n",
    "    bwpppe = ''\n",
    "    recon_mat = ''\n",
    "    pix_band = ''\n",
    "    wfs = ''\n",
    "    etl = ''\n",
    "    red_fact = ''\n",
    "        \n",
    "    if calc_method.lower() == 'dcm':\n",
    "        bwpppe = get_bwpppe(file)\n",
    "        if json_file:\n",
    "            recon_mat = get_recon_mat(json_file)\n",
    "            pix_band = get_pix_band(json_file)\n",
    "            # set bandwidth per pixel to empty if unknown\n",
    "            try:\n",
    "                if bwpppe.lower() == 'unknown':\n",
    "                    bwpppe = ''\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            # set pixel bandwidth to empty if unknown\n",
    "            try:\n",
    "                if pix_band.lower() == 'unknown':\n",
    "                    pix_band = ''\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            etl = recon_mat\n",
    "    elif calc_method.lower() == 'par':\n",
    "        wfs = get_wfs(file)\n",
    "        etl = get_etl(file)\n",
    "        red_fact = get_red_fact(file)\n",
    "        # set water fat shift to empty if unknown\n",
    "        try:\n",
    "            if wfs.lower() == 'unknown':\n",
    "                wfs = ''\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        # set echo train length to empty if unknown\n",
    "        try:\n",
    "            if etl.lower() == 'unknown':\n",
    "                etl = ''\n",
    "        except AttributeError:\n",
    "            pass\n",
    "         # set parallel reduction factor to empty if unknown\n",
    "        try:\n",
    "            if red_fact.lower() == 'unknown':\n",
    "                red_fact = ''\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "    # Calculate effective echo spacing and total readout time\n",
    "    if bwpppe and recon_mat:\n",
    "        eff_echo_sp = 1/(bwpppe * recon_mat)\n",
    "        tot_read_time = eff_echo_sp * (recon_mat - 1)\n",
    "    elif wfs and etl:\n",
    "        if not red_fact:\n",
    "            red_fact = 1\n",
    "        eff_echo_sp = (((1000 * wfs)/(434.215 * (etl + 1)))/red_fact)\n",
    "        tot_read_time = 0.001 * eff_echo_sp * etl\n",
    "    elif pix_band and etl:\n",
    "        eff_echo_sp = ((1/(pix_band * etl)) * (etl - 1)) * 1.3\n",
    "        tot_read_time = eff_echo_sp * (etl - 1)\n",
    "    elif pix_band and recon_mat:\n",
    "        eff_echo_sp = ((1/(pix_band * recon_mat)) * (recon_mat - 1)) * 1.3\n",
    "        tot_read_time = eff_echo_sp * (recon_mat - 1)\n",
    "    else:\n",
    "        eff_echo_sp = \"unknown\"\n",
    "        tot_read_time = \"unknown\"\n",
    "        \n",
    "    return eff_echo_sp,tot_read_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `dcm2niix` Wrapper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_data(file,basename,out_dir,cprss_lvl=6,bids=True,\n",
    "                       anon_bids=True,gzip=True,comment=True,\n",
    "                       adjacent=False,dir_search=5,nrrd=False,\n",
    "                       ignore_2D=True,merge_2D=True,text=False,\n",
    "                       progress=False,verbose=False,\n",
    "                       write_conflicts=\"suffix\",crop_3D=False,\n",
    "                       lossless=False,big_endian=\"optimal\",xml=False):\n",
    "    '''\n",
    "    Converts raw image data (DICOM, PAR REC, or Bruker) to NifTi (or NRRD) using dcm2niix.\n",
    "    This is a wrapper function for dcm2niix (v1.0.20190902+). This wrapper functions has no returns, \n",
    "    however output files are generated in a specified directory that must exist prior to the \n",
    "    invokation of this function.\n",
    "    \n",
    "    Note: Most of the defaults for dcm2niix have been preserved aside from those starred (*) in the\n",
    "    (optional) arguments section, in order to be BIDS compliant.\n",
    "\n",
    "    Arguments (Required):\n",
    "        file (string): Absolute path to raw image data file\n",
    "        basename (string): Output file(s) basename\n",
    "        out_dir (string): Absolute path to output directory (must exist at runtime)\n",
    "\n",
    "    Arguments (Optional):\n",
    "        cprss_lvl (int): Compression level [1 - 9] - 1 is fastest, 9 is smallest (default: 6)\n",
    "        bids (bool): BIDS (JSON) sidecar (default: True) * \n",
    "        anon_bids (bool): Anonymize BIDS (default: True) * \n",
    "        gzip (bool): Gzip compress images (default: True) *\n",
    "        comment (bool): Image comment(s) stored in NifTi header (default: True) *\n",
    "        adjacent (bool): Assumes adjacent DICOMs/Image data (images from same series always in same folder) for faster conversion (default: False)\n",
    "        dir_search (int): Directory search depth (default: 5)\n",
    "        nrrd (bool): Export as NRRD instead of NifTi, not recommended (default: False)\n",
    "        ignore_2D (bool): Ignore derived, localizer and 2D images (default: True)\n",
    "        merge_2D (bool): Merge 2D slices from same series regardless of echo, exposure, etc. (default: True)\n",
    "        text (bool): Text notes includes private patient details in separate text file (default: False)\n",
    "        progress (bool): Report progress, slicer format progress information (default: True)\n",
    "        verbose (bool): Enable verbosity (default: False)\n",
    "        write_conflicts (string): Write behavior for name conflicts:\n",
    "            - 'suffix' = Add suffix to name conflict (default)\n",
    "            - 'overwrite' = Overwrite name conflict\n",
    "            - 'skip' = Skip name conflict\n",
    "        crop_3D (bool): crop 3D acquisitions (default: False)\n",
    "        lossless (bool): Losslessly scale 16-bit integers to use dynamic range (default: True)\n",
    "        big_endian (string): Byte order:\n",
    "            - 'optimal' or 'native' = optimal/native byte order (default)\n",
    "            - 'little-end' = little endian\n",
    "            - 'big-end' = big endian\n",
    "        xml (bool): Slicer format features (default: False)\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "    '''\n",
    "\n",
    "    # Empty list\n",
    "    conv_cmd = list()\n",
    "\n",
    "    # Get OS platform\n",
    "    if platform.system().lower() == 'windows':\n",
    "        conv_cmd.append(\"dcm2niix.exe\")\n",
    "    else:\n",
    "        conv_cmd.append(\"dcm2niix\")\n",
    "\n",
    "    # Boolean True/False options arrays\n",
    "    bool_opts = [bids, anon_bids, gzip, comment, adjacent, nrrd, ignore_2D, merge_2D, text, verbose, lossless, progress, xml]\n",
    "    bool_vars = [\"-b\", \"-ba\", \"-z\", \"-c\", \"-a\", \"-e\", \"-i\", \"-m\", \"-t\", \"-v\", \"-l\", \"--progress\", \"--xml\"]\n",
    "\n",
    "    # Initial option(s)\n",
    "    if cprss_lvl:\n",
    "        conv_cmd.append(f\"-{cprss_lvl}\")\n",
    "\n",
    "    # Required option(s)\n",
    "    if basename:\n",
    "        conv_cmd.append(\"-f\")\n",
    "        conv_cmd.append(f\"{basename}\")\n",
    "\n",
    "    if basename:\n",
    "        conv_cmd.append(\"-f\")\n",
    "        conv_cmd.append(f\"{basename}\")\n",
    "\n",
    "    if out_dir:\n",
    "        conv_cmd.append(\"-o\")\n",
    "        conv_cmd.append(f\"{out_dir}\")\n",
    "\n",
    "    # Keyword option(s)\n",
    "    if write_conflicts.lower() == \"suffix\":\n",
    "        conv_cmd.append(\"-w\")\n",
    "        conv_cmd.append(\"2\")\n",
    "    elif write_conflicts.lower() == \"overwrite\":\n",
    "        conv_cmd.append(\"-w\")\n",
    "        conv_cmd.append(\"1\")\n",
    "    elif write_conflicts.lower() == \"skip\":\n",
    "        conv_cmd.append(\"-w\")\n",
    "        conv_cmd.append(\"0\")\n",
    "\n",
    "    if big_endian.lower() == \"optimal\" or big_endian.lower() == \"native\":\n",
    "        conv_cmd.append(\"--big_endian\")\n",
    "        conv_cmd.append(\"o\")\n",
    "    elif big_endian.lower() == \"little-end\":\n",
    "        conv_cmd.append(\"--big_endian\")\n",
    "        conv_cmd.append(\"n\")\n",
    "    elif big_endian.lower() == \"big-end\":\n",
    "        conv_cmd.append(\"--big_endian\")\n",
    "        conv_cmd.append(\"y\")\n",
    "\n",
    "\n",
    "    for idx,var in enumerate(bool_opts):\n",
    "        if var:\n",
    "            conv_cmd.append(bool_vars[idx])\n",
    "            conv_cmd.append(\"y\")\n",
    "\n",
    "    # Required arguments\n",
    "    # Filename\n",
    "    conv_cmd.append(\"-f\")\n",
    "    conv_cmd.append(f\"{basename}\")\n",
    "\n",
    "    # Output directory\n",
    "    conv_cmd.append(\"-o\")\n",
    "    conv_cmd.append(f\"{out_dir}\")\n",
    "\n",
    "    # Image file   \n",
    "    conv_cmd.append(f\"{file}\")\n",
    "\n",
    "    # System Call to dcm2niix (assumes dcm2niix is added to system path variable)\n",
    "    subprocess.call(conv_cmd)\n",
    "    \n",
    "    return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
